{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37464bitcontinuumvirtualenv48009ba53764420baffbdb8297af855b",
   "display_name": "Python 3.7.4 64-bit ('Continuum': virtualenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using TensorFlow backend.\n"
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# ML imports\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, Activation\n",
    "import keras.backend as K\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import initializers\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from statsmodels.tsa.stattools import coint\n",
    "\n",
    "\n",
    "def data_preprocess(dta):\n",
    "    dta['Date'] = pd.to_datetime(dta['Date'], format='%Y-%m-%d')\n",
    "    dta = dta.set_index(dta['Date'])\n",
    "    # NHLI not traded\n",
    "    dta.drop(['Date', 'NHLI'], axis=1, inplace=True)\n",
    "    dta.dropna(how='all', inplace=True)\n",
    "    for tick in dta.columns:\n",
    "        tick_series = dta[tick]\n",
    "        start_pos = tick_series.first_valid_index()\n",
    "        valid_series = tick_series.loc[start_pos:]\n",
    "        if valid_series.isna().sum() > 0:\n",
    "            dta.drop(tick, axis=1, inplace=True)\n",
    "\n",
    "    for tick in dta.columns:\n",
    "        dta[tick] = dta[tick].mask(dta[tick] == 0).ffill(downcast='infer')\n",
    "\n",
    "    return dta[dta.index >= dta['SPY'].first_valid_index()]\n",
    "\n",
    "\n",
    "def coeff_deter(y_true, y_pred):\n",
    "    SS_res = K.sum(K.square(y_true - y_pred) * 1e6)\n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true)) * 1e6)\n",
    "    return 1 - SS_res / (SS_tot + K.epsilon())\n",
    "\n",
    "\n",
    "def coint_group(tick, dta):\n",
    "    \"\"\"\n",
    "    Use cointegration test and correlation to find predictive stocks for target\n",
    "    :param tick: string for the target stock\n",
    "    :param dta: the data file (csv) that contains the tick\n",
    "    :return: a list of tickers that are in sp500 which predict the target\n",
    "    \"\"\"\n",
    "    dta['%s_LAG' % tick] = dta[tick].shift(-120)\n",
    "    dta.dropna(inplace=True)\n",
    "\n",
    "    y = dta['%s_LAG' % tick]\n",
    "    cointegrat = {}\n",
    "    correlat = {}\n",
    "\n",
    "    for i in dta.columns[:-2]:\n",
    "        x = dta[i]\n",
    "        score, pval, _ = coint(x, y, trend='ct')\n",
    "        corr = x.corr(y)\n",
    "\n",
    "        cointegrat[i] = pval\n",
    "        correlat[i] = corr\n",
    "\n",
    "    best_coint = sorted(cointegrat, key=cointegrat.get)[:10]\n",
    "    best_corr = sorted(correlat, key=correlat.get, reverse=True)[:10]\n",
    "\n",
    "    intersect = list(set(best_coint) & set(best_corr))\n",
    "    if len(intersect) > 0:\n",
    "        print(\"There are {} cointegrated stocks.\".format(len(intersect)))\n",
    "        return intersect\n",
    "    else:\n",
    "        print(\"Intersection is empty.\")\n",
    "        return best_coint[:3]\n",
    "\n",
    "\n",
    "def measure_profit(tick, fitted_val, asset, dta):\n",
    "    inventory = 0\n",
    "    asset = asset\n",
    "    record = [asset]\n",
    "    forecast_diff = fitted_val\n",
    "    T = min(len(forecast_diff), len(dta))\n",
    "\n",
    "    for t in range(T):\n",
    "        trend_good = forecast_diff[t] > dta[tick].iloc[t]\n",
    "        price = dta[tick].iloc[t]\n",
    "        if trend_good and inventory == 0:\n",
    "            # buy\n",
    "            asset -= price\n",
    "            inventory += 1\n",
    "        elif not trend_good and inventory == 1:\n",
    "            # sell\n",
    "            asset += price\n",
    "            inventory -= 1\n",
    "        elif t == len(forecast_diff) - 1 and inventory == 1:\n",
    "            asset += price\n",
    "            inventory -= 1\n",
    "        else:\n",
    "            asset = record[-1]\n",
    "        record.append(asset)\n",
    "\n",
    "    return asset, record[1:]\n",
    "\n",
    "\n",
    "def regression_mod(X, Y, dta):\n",
    "    \"\"\"\n",
    "    Use basic regression model to forecast\n",
    "    :param X: list of strings of tickers\n",
    "    :param Y: string of lagged target ticker\n",
    "    :param dta: the data set that contains X and Y\n",
    "    :return: the regression model (statsmodels mod format)\n",
    "    \"\"\"\n",
    "    X = dta[X]\n",
    "    Y = dta[Y]\n",
    "    mod = sm.OLS(Y, sm.add_constant(X)).fit()\n",
    "    return mod\n",
    "\n",
    "\n",
    "def LSTM_mod(X, Y, scaler_x, scaler_y):\n",
    "    \"\"\"\n",
    "    To adjust lstm machine learning model architecture (layers, activations, kernels...)\n",
    "    :param X: np arrays\n",
    "    :param Y: np array (1 dimensional)\n",
    "    :param scaler_x: a scaler class from sklearn (unfitted)\n",
    "    :param scaler_y: a scaler class from sklearn (unfitted)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.4, random_state=42)\n",
    "\n",
    "    scaler_x = scaler_x.fit(X_train)\n",
    "    scaler_y = scaler_y.fit(Y_train)\n",
    "\n",
    "    X_train = scaler_x.transform(X_train)\n",
    "    Y_train = scaler_y.transform(Y_train)\n",
    "\n",
    "    X_test = scaler_x.transform(X_test)\n",
    "    Y_test = scaler_y.transform(Y_test)\n",
    "\n",
    "    X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "    X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "\n",
    "    initializer = initializers.glorot_normal(seed=42)\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(20, input_shape=(X_train.shape[1], X_train.shape[2]), kernel_initializer=initializer))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(10, kernel_initializer=initializer))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(1, kernel_initializer=initializer))\n",
    "    model.compile(loss='mae', optimizer='adam', metrics=[coeff_deter])\n",
    "\n",
    "    es = EarlyStopping(monitor='val_coeff_deter', mode='max', patience=5)\n",
    "\n",
    "    model.fit(X_train, Y_train,\n",
    "              batch_size=32,\n",
    "              validation_data=(X_test, Y_test),\n",
    "              epochs=50,\n",
    "              callbacks=[es],\n",
    "              verbose=2)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = pd.read_csv('sp500_stock.csv')\n",
    "data = pd.read_csv('broader_stock.csv')\n",
    "\n",
    "sp = data_preprocess(sp)\n",
    "data = data_preprocess(data)\n",
    "\n",
    "ticker_list = list(data.columns)\n",
    "ticker_list.remove('SPY')\n",
    "\n",
    "tick = 'MSFT'\n",
    "original_series = data[tick]\n",
    "\n",
    "if tick in sp.columns:\n",
    "    original_data = pd.concat([sp.drop([tick], axis=1), original_series], axis=1)\n",
    "    original_data = original_data[original_data[tick].notnull()].dropna(axis=1)\n",
    "else:\n",
    "    original_data = pd.concat([sp, original_series], axis=1)\n",
    "    original_data = original_data[original_data[tick].notnull()].dropna(axis=1)\n",
    "\n",
    "cutoff = int(original_data.shape[0] * 0.8)\n",
    "observed_data = original_data.iloc[:cutoff]\n",
    "\n",
    "arr = observed_data[tick]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coint_corr = coint_group(tick, observed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(5399, 120) (5399, 1)\nTrain on 3239 samples, validate on 2160 samples\nEpoch 1/50\n - 1s - loss: 0.4285 - coeff_deter: 0.6205 - val_loss: 0.2862 - val_coeff_deter: 0.8589\nEpoch 2/50\n - 0s - loss: 0.3777 - coeff_deter: 0.7042 - val_loss: 0.2608 - val_coeff_deter: 0.8686\nEpoch 3/50\n - 1s - loss: 0.3568 - coeff_deter: 0.7366 - val_loss: 0.2927 - val_coeff_deter: 0.8511\nEpoch 4/50\n - 0s - loss: 0.3452 - coeff_deter: 0.7564 - val_loss: 0.2808 - val_coeff_deter: 0.8687\nEpoch 5/50\n - 0s - loss: 0.3371 - coeff_deter: 0.7584 - val_loss: 0.2312 - val_coeff_deter: 0.8987\nEpoch 6/50\n - 0s - loss: 0.3269 - coeff_deter: 0.7798 - val_loss: 0.2300 - val_coeff_deter: 0.9051\nEpoch 7/50\n - 0s - loss: 0.3297 - coeff_deter: 0.7774 - val_loss: 0.2125 - val_coeff_deter: 0.9089\nEpoch 8/50\n - 0s - loss: 0.3244 - coeff_deter: 0.7835 - val_loss: 0.1965 - val_coeff_deter: 0.9186\nEpoch 9/50\n - 1s - loss: 0.3229 - coeff_deter: 0.7825 - val_loss: 0.2784 - val_coeff_deter: 0.8693\nEpoch 10/50\n - 0s - loss: 0.3203 - coeff_deter: 0.7891 - val_loss: 0.2083 - val_coeff_deter: 0.9081\nEpoch 11/50\n - 0s - loss: 0.3154 - coeff_deter: 0.7939 - val_loss: 0.2541 - val_coeff_deter: 0.8914\nEpoch 12/50\n - 0s - loss: 0.3135 - coeff_deter: 0.7967 - val_loss: 0.1962 - val_coeff_deter: 0.9216\nEpoch 13/50\n - 0s - loss: 0.3103 - coeff_deter: 0.7999 - val_loss: 0.2209 - val_coeff_deter: 0.9125\nEpoch 14/50\n - 1s - loss: 0.3103 - coeff_deter: 0.8010 - val_loss: 0.2018 - val_coeff_deter: 0.9161\nEpoch 15/50\n - 1s - loss: 0.3129 - coeff_deter: 0.7858 - val_loss: 0.2367 - val_coeff_deter: 0.9029\nEpoch 16/50\n - 0s - loss: 0.3100 - coeff_deter: 0.7942 - val_loss: 0.2376 - val_coeff_deter: 0.9001\nEpoch 17/50\n - 0s - loss: 0.3047 - coeff_deter: 0.8048 - val_loss: 0.1815 - val_coeff_deter: 0.9308\nEpoch 18/50\n - 0s - loss: 0.3019 - coeff_deter: 0.8084 - val_loss: 0.2446 - val_coeff_deter: 0.8988\nEpoch 19/50\n - 0s - loss: 0.3082 - coeff_deter: 0.8019 - val_loss: 0.2478 - val_coeff_deter: 0.8965\nEpoch 20/50\n - 0s - loss: 0.3122 - coeff_deter: 0.8001 - val_loss: 0.2319 - val_coeff_deter: 0.8936\nEpoch 21/50\n - 0s - loss: 0.3071 - coeff_deter: 0.8009 - val_loss: 0.2465 - val_coeff_deter: 0.8990\nEpoch 22/50\n - 0s - loss: 0.3005 - coeff_deter: 0.8057 - val_loss: 0.1888 - val_coeff_deter: 0.9293\n"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# machine learning model\n",
    "train_length = 30\n",
    "X, Y = [], []\n",
    "\n",
    "for i in range(len(observed_data) - train_length):\n",
    "    x = observed_data[coint_corr].iloc[i:i + train_length].values.T.flatten()\n",
    "    y = observed_data['%s_LAG' % tick].iloc[i + train_length]\n",
    "    X.append(x)\n",
    "    Y.append(y)\n",
    "\n",
    "X = np.array(X)\n",
    "Y = np.array(Y).reshape(-1, 1)\n",
    "\n",
    "print(X.shape, Y.shape)\n",
    "mm_scaler_x = StandardScaler()\n",
    "mm_scaler_y = StandardScaler()\n",
    "lstm_mod = LSTM_mod(X, Y, mm_scaler_x, mm_scaler_y)\n",
    "\n",
    "mm_scaler_x = mm_scaler_x.fit(X)\n",
    "scale = mm_scaler_x.transform(X)\n",
    "scale = scale.reshape(scale.shape[0], 1, scale.shape[1])\n",
    "\n",
    "y_pred = lstm_mod.predict(scale)\n",
    "mm_scaler_y = mm_scaler_y.fit(Y)\n",
    "y_pred = mm_scaler_y.inverse_transform(y_pred)\n",
    "\n",
    "# examine trading profit\n",
    "mlasset, mlrecord = measure_profit(tick, y_pred.flatten(), 0, observed_data.shift(-train_length).iloc[:-train_length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "80.64"
     },
     "metadata": {},
     "execution_count": 149
    }
   ],
   "source": [
    "mlasset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[<matplotlib.lines.Line2D at 0x2984d79e888>]"
     },
     "metadata": {},
     "execution_count": 150
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 368.925 248.518125\" width=\"368.925pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M -0 248.518125 \r\nL 368.925 248.518125 \r\nL 368.925 0 \r\nL -0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 26.925 224.64 \r\nL 361.725 224.64 \r\nL 361.725 7.2 \r\nL 26.925 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m939ac56987\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"42.143182\" xlink:href=\"#m939ac56987\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n      </defs>\r\n      <g transform=\"translate(38.961932 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"98.527701\" xlink:href=\"#m939ac56987\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 1000 -->\r\n      <defs>\r\n       <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n      </defs>\r\n      <g transform=\"translate(85.802701 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"154.912221\" xlink:href=\"#m939ac56987\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 2000 -->\r\n      <defs>\r\n       <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n      </defs>\r\n      <g transform=\"translate(142.187221 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"211.29674\" xlink:href=\"#m939ac56987\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 3000 -->\r\n      <defs>\r\n       <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n      </defs>\r\n      <g transform=\"translate(198.57174 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"267.68126\" xlink:href=\"#m939ac56987\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 4000 -->\r\n      <defs>\r\n       <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n      </defs>\r\n      <g transform=\"translate(254.95626 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"324.065779\" xlink:href=\"#m939ac56987\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 5000 -->\r\n      <defs>\r\n       <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n      </defs>\r\n      <g transform=\"translate(311.340779 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m74b8a386f0\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m74b8a386f0\" y=\"204.506667\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(13.5625 208.305885)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m74b8a386f0\" y=\"158.02278\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 20 -->\r\n      <g transform=\"translate(7.2 161.821999)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m74b8a386f0\" y=\"111.538894\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 40 -->\r\n      <g transform=\"translate(7.2 115.338112)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m74b8a386f0\" y=\"65.055007\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 60 -->\r\n      <defs>\r\n       <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 68.854226)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-54\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m74b8a386f0\" y=\"18.571121\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 80 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 34.625 \r\nQ 24.75 34.625 20.71875 30.859375 \r\nQ 16.703125 27.09375 16.703125 20.515625 \r\nQ 16.703125 13.921875 20.71875 10.15625 \r\nQ 24.75 6.390625 31.78125 6.390625 \r\nQ 38.8125 6.390625 42.859375 10.171875 \r\nQ 46.921875 13.96875 46.921875 20.515625 \r\nQ 46.921875 27.09375 42.890625 30.859375 \r\nQ 38.875 34.625 31.78125 34.625 \r\nz\r\nM 21.921875 38.8125 \r\nQ 15.578125 40.375 12.03125 44.71875 \r\nQ 8.5 49.078125 8.5 55.328125 \r\nQ 8.5 64.0625 14.71875 69.140625 \r\nQ 20.953125 74.21875 31.78125 74.21875 \r\nQ 42.671875 74.21875 48.875 69.140625 \r\nQ 55.078125 64.0625 55.078125 55.328125 \r\nQ 55.078125 49.078125 51.53125 44.71875 \r\nQ 48 40.375 41.703125 38.8125 \r\nQ 48.828125 37.15625 52.796875 32.3125 \r\nQ 56.78125 27.484375 56.78125 20.515625 \r\nQ 56.78125 9.90625 50.3125 4.234375 \r\nQ 43.84375 -1.421875 31.78125 -1.421875 \r\nQ 19.734375 -1.421875 13.25 4.234375 \r\nQ 6.78125 9.90625 6.78125 20.515625 \r\nQ 6.78125 27.484375 10.78125 32.3125 \r\nQ 14.796875 37.15625 21.921875 38.8125 \r\nz\r\nM 18.3125 54.390625 \r\nQ 18.3125 48.734375 21.84375 45.5625 \r\nQ 25.390625 42.390625 31.78125 42.390625 \r\nQ 38.140625 42.390625 41.71875 45.5625 \r\nQ 45.3125 48.734375 45.3125 54.390625 \r\nQ 45.3125 60.0625 41.71875 63.234375 \r\nQ 38.140625 66.40625 31.78125 66.40625 \r\nQ 25.390625 66.40625 21.84375 63.234375 \r\nQ 18.3125 60.0625 18.3125 54.390625 \r\nz\r\n\" id=\"DejaVuSans-56\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 22.370339)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-56\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_12\">\r\n    <path clip-path=\"url(#p4e448b1c6c)\" d=\"M 42.143182 208.574007 \r\nL 52.34878 208.574007 \r\nL 52.461549 204.78557 \r\nL 69.433289 204.78557 \r\nL 69.546058 210.549572 \r\nL 70.109903 210.549572 \r\nL 70.166288 204.622876 \r\nL 70.222673 210.52633 \r\nL 70.335442 204.553151 \r\nL 78.736735 204.553151 \r\nL 78.849504 212.269476 \r\nL 79.075042 212.269476 \r\nL 79.187811 203.809408 \r\nL 79.92081 203.809408 \r\nL 80.033579 212.455411 \r\nL 80.089963 203.251602 \r\nL 80.202732 212.246234 \r\nL 81.161269 212.246234 \r\nL 81.217654 203.460779 \r\nL 81.330423 212.106782 \r\nL 86.179492 212.106782 \r\nL 86.292261 202.252198 \r\nL 91.705174 202.252198 \r\nL 91.761559 213.547783 \r\nL 91.874328 202.15923 \r\nL 93.39671 202.15923 \r\nL 93.509479 214.756364 \r\nL 100.952236 214.756364 \r\nL 101.065005 192.165195 \r\nL 101.177774 192.165195 \r\nL 101.290543 213.966138 \r\nL 101.967157 213.966138 \r\nL 102.079926 190.422049 \r\nL 102.192695 213.826686 \r\nL 125.423117 213.826686 \r\nL 125.479502 153.374392 \r\nL 125.592271 212.222992 \r\nL 125.70504 152.049601 \r\nL 126.043347 152.049601 \r\nL 126.156116 211.177104 \r\nL 128.07319 211.177104 \r\nL 128.185959 142.497162 \r\nL 128.242343 209.131813 \r\nL 128.355112 140.219452 \r\nL 128.637035 140.219452 \r\nL 128.749804 207.225974 \r\nL 131.287107 207.225974 \r\nL 131.399876 144.03113 \r\nL 132.978643 144.03113 \r\nL 133.091412 207.086522 \r\nL 133.824411 207.086522 \r\nL 133.93718 142.822549 \r\nL 143.015087 142.822549 \r\nL 143.127856 201.740875 \r\nL 143.240625 145.332679 \r\nL 143.353394 145.332679 \r\nL 143.466163 194.86126 \r\nL 143.635317 194.86126 \r\nL 143.748086 140.265936 \r\nL 144.030009 140.265936 \r\nL 144.142778 189.469129 \r\nL 144.255547 138.313612 \r\nL 144.368316 138.313612 \r\nL 144.481085 187.516806 \r\nL 144.988545 187.516806 \r\nL 145.101314 137.80229 \r\nL 149.893999 137.80229 \r\nL 150.006768 179.126465 \r\nL 150.401459 179.126465 \r\nL 150.514228 133.130659 \r\nL 150.626997 133.130659 \r\nL 150.683382 178.661626 \r\nL 150.796151 130.759981 \r\nL 151.980226 130.759981 \r\nL 152.092995 179.126465 \r\nL 155.701604 179.126465 \r\nL 155.757989 134.013853 \r\nL 155.870758 178.057335 \r\nL 156.378218 178.057335 \r\nL 156.434603 136.01266 \r\nL 156.490987 177.708706 \r\nL 156.547372 134.408966 \r\nL 156.603756 175.733141 \r\nL 157.054833 175.733141 \r\nL 157.167602 131.364271 \r\nL 161.734748 131.364271 \r\nL 161.791132 179.591304 \r\nL 161.903901 131.248062 \r\nL 162.01667 131.248062 \r\nL 162.073055 178.243271 \r\nL 162.129439 130.202174 \r\nL 162.185824 176.221222 \r\nL 163.708206 176.221222 \r\nL 163.820975 134.408966 \r\nL 163.933744 134.408966 \r\nL 163.990128 174.966157 \r\nL 164.102898 133.688466 \r\nL 171.99673 133.688466 \r\nL 172.053115 169.829687 \r\nL 172.165884 133.037691 \r\nL 173.068036 133.037691 \r\nL 173.180805 169.783203 \r\nL 173.293574 131.364271 \r\nL 175.041494 131.364271 \r\nL 175.097879 163.345185 \r\nL 175.267032 127.087754 \r\nL 175.49257 127.087754 \r\nL 175.60534 160.114555 \r\nL 175.661724 160.114555 \r\nL 175.774493 125.112189 \r\nL 177.466029 125.112189 \r\nL 177.578798 158.74328 \r\nL 177.691567 124.159269 \r\nL 177.804336 157.813603 \r\nL 177.917105 123.438769 \r\nL 178.029874 156.744473 \r\nL 178.086258 156.744473 \r\nL 178.142643 123.299317 \r\nL 178.199027 156.000731 \r\nL 178.311796 121.509687 \r\nL 180.736331 121.509687 \r\nL 180.792715 160.556152 \r\nL 180.905484 120.045445 \r\nL 181.018253 159.835652 \r\nL 181.582099 159.835652 \r\nL 181.638483 118.348783 \r\nL 181.694868 158.673555 \r\nL 181.807637 117.186686 \r\nL 182.033175 117.186686 \r\nL 182.145944 155.442924 \r\nL 184.175786 155.442924 \r\nL 184.288556 118.348783 \r\nL 184.514094 118.348783 \r\nL 184.626863 156.070457 \r\nL 184.908785 156.070457 \r\nL 185.021554 117.698009 \r\nL 185.134323 155.117537 \r\nL 185.190708 116.954266 \r\nL 185.247092 153.606811 \r\nL 190.941929 153.606811 \r\nL 190.998313 111.190265 \r\nL 191.054698 153.537085 \r\nL 191.111082 110.516248 \r\nL 191.223851 152.839827 \r\nL 191.33662 152.839827 \r\nL 191.44939 110.53949 \r\nL 191.731312 110.53949 \r\nL 191.844081 153.00252 \r\nL 191.900466 153.00252 \r\nL 191.95685 109.981684 \r\nL 192.013235 151.44531 \r\nL 192.126004 108.889312 \r\nL 192.971772 108.889312 \r\nL 193.084541 148.818971 \r\nL 196.411227 148.818971 \r\nL 196.523996 106.100279 \r\nL 196.636765 148.470342 \r\nL 196.69315 148.470342 \r\nL 196.805919 106.983473 \r\nL 197.256995 106.983473 \r\nL 197.31338 146.866647 \r\nL 197.426149 106.379182 \r\nL 197.538918 146.076421 \r\nL 199.963452 146.076421 \r\nL 200.076221 104.752246 \r\nL 200.132606 104.752246 \r\nL 200.245375 144.588937 \r\nL 206.222134 144.588937 \r\nL 206.278518 102.079423 \r\nL 206.334903 144.379759 \r\nL 206.447672 101.68431 \r\nL 206.67321 101.68431 \r\nL 206.785979 143.868437 \r\nL 206.842363 143.868437 \r\nL 206.955132 101.126503 \r\nL 207.067901 101.126503 \r\nL 207.18067 142.869033 \r\nL 207.518978 142.869033 \r\nL 207.631747 100.034132 \r\nL 210.281819 100.034132 \r\nL 210.394588 144.077614 \r\nL 210.78928 144.077614 \r\nL 210.902049 100.289793 \r\nL 211.409509 100.289793 \r\nL 211.465894 143.845195 \r\nL 211.578663 100.220067 \r\nL 211.747817 100.220067 \r\nL 211.804201 142.659856 \r\nL 211.91697 100.266551 \r\nL 212.029739 142.752823 \r\nL 212.086124 100.220067 \r\nL 212.198893 142.70634 \r\nL 212.255277 142.70634 \r\nL 212.311662 99.871438 \r\nL 212.424431 142.59013 \r\nL 214.059582 142.59013 \r\nL 214.115966 100.057374 \r\nL 214.228735 142.078807 \r\nL 214.28512 142.078807 \r\nL 214.341504 100.034132 \r\nL 214.397889 141.172371 \r\nL 214.510658 98.616373 \r\nL 216.258578 98.616373 \r\nL 216.371347 140.870226 \r\nL 217.329884 140.870226 \r\nL 217.386268 97.035921 \r\nL 217.499038 140.568081 \r\nL 217.611807 140.568081 \r\nL 217.668191 95.94355 \r\nL 217.78096 139.289774 \r\nL 218.062883 139.289774 \r\nL 218.175652 94.03771 \r\nL 219.52888 94.03771 \r\nL 219.585265 139.080596 \r\nL 219.641649 94.084194 \r\nL 219.754418 139.12708 \r\nL 221.558723 139.12708 \r\nL 221.671492 95.920308 \r\nL 225.900331 95.920308 \r\nL 225.956715 141.125887 \r\nL 226.0131 95.618163 \r\nL 226.125869 141.03292 \r\nL 226.182254 94.967388 \r\nL 226.238638 140.730774 \r\nL 226.295023 94.502549 \r\nL 226.407792 140.428629 \r\nL 226.464176 140.428629 \r\nL 226.576945 94.339856 \r\nL 227.084406 94.339856 \r\nL 227.197175 140.591323 \r\nL 227.591867 140.591323 \r\nL 227.704636 93.503146 \r\nL 227.817405 140.451871 \r\nL 234.7527 140.451871 \r\nL 234.86547 92.503742 \r\nL 234.921854 92.503742 \r\nL 235.034623 140.08 \r\nL 235.091008 140.08 \r\nL 235.203777 91.64379 \r\nL 235.37293 91.64379 \r\nL 235.429315 140.19621 \r\nL 235.542084 91.411371 \r\nL 235.654853 139.940548 \r\nL 236.218698 139.940548 \r\nL 236.331467 89.714709 \r\nL 237.290004 89.714709 \r\nL 237.346388 139.405984 \r\nL 237.459157 88.785031 \r\nL 239.432616 88.785031 \r\nL 239.545385 139.777855 \r\nL 239.827307 139.777855 \r\nL 239.940076 88.970967 \r\nL 240.052845 138.639 \r\nL 241.180536 138.639 \r\nL 241.23692 89.36608 \r\nL 241.349689 138.220645 \r\nL 241.744381 138.220645 \r\nL 241.800765 88.970967 \r\nL 241.913534 138.313612 \r\nL 245.860451 138.313612 \r\nL 245.916835 83.695046 \r\nL 246.029604 137.709322 \r\nL 249.750983 137.709322 \r\nL 249.863752 82.06811 \r\nL 254.600051 82.06811 \r\nL 254.71282 129.737335 \r\nL 254.825589 81.394093 \r\nL 260.125734 81.394093 \r\nL 260.182119 126.483463 \r\nL 260.294888 81.11519 \r\nL 260.351272 125.855931 \r\nL 260.407657 79.860125 \r\nL 260.464041 124.717076 \r\nL 260.57681 78.790996 \r\nL 260.689579 78.790996 \r\nL 260.745964 123.252833 \r\nL 260.858733 77.16406 \r\nL 261.873654 77.16406 \r\nL 261.986423 124.670592 \r\nL 262.268346 124.670592 \r\nL 262.324731 75.908995 \r\nL 262.4375 123.252833 \r\nL 262.832191 123.252833 \r\nL 262.888576 74.863107 \r\nL 262.94496 119.022799 \r\nL 263.452421 119.022799 \r\nL 263.508805 74.003155 \r\nL 263.56519 116.559153 \r\nL 264.354573 116.559153 \r\nL 264.410958 75.002559 \r\nL 264.467342 113.979298 \r\nL 274.447402 113.979298 \r\nL 274.503787 67.937008 \r\nL 274.560171 110.167619 \r\nL 278.056011 110.167619 \r\nL 278.168781 62.010313 \r\nL 286.288151 62.010313 \r\nL 286.40092 110.237345 \r\nL 293.900061 110.237345 \r\nL 293.956446 60.057989 \r\nL 294.069215 109.911958 \r\nL 294.181984 59.337489 \r\nL 297.62144 59.337489 \r\nL 297.734209 106.797537 \r\nL 297.790593 58.802924 \r\nL 297.903362 106.239731 \r\nL 298.185285 106.239731 \r\nL 298.241669 58.012698 \r\nL 298.354439 105.86786 \r\nL 298.692746 105.86786 \r\nL 298.805515 57.013295 \r\nL 298.861899 57.013295 \r\nL 298.974668 105.542472 \r\nL 299.425744 105.542472 \r\nL 299.538513 56.618182 \r\nL 299.594898 56.618182 \r\nL 299.707667 105.03115 \r\nL 302.24497 105.03115 \r\nL 302.357739 54.689101 \r\nL 303.372661 54.689101 \r\nL 303.48543 103.473939 \r\nL 304.331198 103.473939 \r\nL 304.443967 54.572891 \r\nL 304.556736 54.572891 \r\nL 304.669505 103.404214 \r\nL 304.782274 54.20102 \r\nL 304.895043 102.916133 \r\nL 304.951427 102.916133 \r\nL 305.064196 52.760019 \r\nL 305.23335 52.760019 \r\nL 305.346119 101.939971 \r\nL 305.966349 101.939971 \r\nL 306.079118 50.97039 \r\nL 306.248271 50.97039 \r\nL 306.304656 102.032939 \r\nL 306.417425 50.342857 \r\nL 306.530194 50.342857 \r\nL 306.642963 101.800519 \r\nL 309.405804 101.800519 \r\nL 309.462189 48.204598 \r\nL 309.518573 101.103261 \r\nL 309.574958 48.018663 \r\nL 309.631342 100.870842 \r\nL 309.744111 47.484098 \r\nL 316.679407 47.484098 \r\nL 316.735792 102.799923 \r\nL 316.848561 46.019856 \r\nL 319.83694 46.019856 \r\nL 319.949709 103.35773 \r\nL 320.062479 45.299355 \r\nL 320.175248 103.334488 \r\nL 344.928052 103.334488 \r\nL 345.040821 17.083636 \r\nL 346.506818 17.083636 \r\nL 346.506818 17.083636 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 26.925 224.64 \r\nL 26.925 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 361.725 224.64 \r\nL 361.725 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 26.925 224.64 \r\nL 361.725 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 26.925 7.2 \r\nL 361.725 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p4e448b1c6c\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"26.925\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "plt.plot(mlrecord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = original_data.iloc[cutoff:]\n",
    "init_asset = 0\n",
    "mlrecord_os = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = test_data.shape[0] // 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "ated stocks.\nTrain on 3311 samples, validate on 2208 samples\nEpoch 1/50\n - 1s - loss: 0.4268 - coeff_deter: 0.6155 - val_loss: 0.2724 - val_coeff_deter: 0.8516\nEpoch 2/50\n - 1s - loss: 0.3776 - coeff_deter: 0.6916 - val_loss: 0.2890 - val_coeff_deter: 0.8467\nEpoch 3/50\n - 1s - loss: 0.3500 - coeff_deter: 0.7392 - val_loss: 0.2457 - val_coeff_deter: 0.8848\nEpoch 4/50\n - 1s - loss: 0.3423 - coeff_deter: 0.7470 - val_loss: 0.2134 - val_coeff_deter: 0.9087\nEpoch 5/50\n - 1s - loss: 0.3271 - coeff_deter: 0.7720 - val_loss: 0.1910 - val_coeff_deter: 0.9244\nEpoch 6/50\n - 1s - loss: 0.3180 - coeff_deter: 0.7849 - val_loss: 0.2126 - val_coeff_deter: 0.9100\nEpoch 7/50\n - 1s - loss: 0.3279 - coeff_deter: 0.7610 - val_loss: 0.1978 - val_coeff_deter: 0.9233\nEpoch 8/50\n - 1s - loss: 0.3191 - coeff_deter: 0.7804 - val_loss: 0.2202 - val_coeff_deter: 0.9074\nEpoch 9/50\n - 1s - loss: 0.3183 - coeff_deter: 0.7746 - val_loss: 0.2319 - val_coeff_deter: 0.9053\nEpoch 10/50\n - 0s - loss: 0.3093 - coeff_deter: 0.7887 - val_loss: 0.1904 - val_coeff_deter: 0.9258\nEpoch 11/50\n - 1s - loss: 0.3083 - coeff_deter: 0.7867 - val_loss: 0.2453 - val_coeff_deter: 0.8902\nEpoch 12/50\n - 1s - loss: 0.3072 - coeff_deter: 0.7884 - val_loss: 0.1952 - val_coeff_deter: 0.9251\nEpoch 13/50\n - 1s - loss: 0.3056 - coeff_deter: 0.7930 - val_loss: 0.1797 - val_coeff_deter: 0.9309\nEpoch 14/50\n - 1s - loss: 0.3005 - coeff_deter: 0.8057 - val_loss: 0.1855 - val_coeff_deter: 0.9312\nEpoch 15/50\n - 1s - loss: 0.3116 - coeff_deter: 0.7749 - val_loss: 0.1785 - val_coeff_deter: 0.9347\nEpoch 16/50\n - 1s - loss: 0.3016 - coeff_deter: 0.7962 - val_loss: 0.2272 - val_coeff_deter: 0.9069\nEpoch 17/50\n - 1s - loss: 0.3014 - coeff_deter: 0.8028 - val_loss: 0.1893 - val_coeff_deter: 0.9339\nEpoch 18/50\n - 1s - loss: 0.3022 - coeff_deter: 0.7883 - val_loss: 0.1892 - val_coeff_deter: 0.9275\nEpoch 19/50\n - 1s - loss: 0.3017 - coeff_deter: 0.7943 - val_loss: 0.1725 - val_coeff_deter: 0.9368\nEpoch 20/50\n - 0s - loss: 0.2857 - coeff_deter: 0.8098 - val_loss: 0.1668 - val_coeff_deter: 0.9393\nEpoch 21/50\n - 1s - loss: 0.2932 - coeff_deter: 0.8033 - val_loss: 0.1793 - val_coeff_deter: 0.9385\nEpoch 22/50\n - 1s - loss: 0.2958 - coeff_deter: 0.8012 - val_loss: 0.1879 - val_coeff_deter: 0.9244\nEpoch 23/50\n - 1s - loss: 0.2970 - coeff_deter: 0.7972 - val_loss: 0.1641 - val_coeff_deter: 0.9341\nEpoch 24/50\n - 1s - loss: 0.3013 - coeff_deter: 0.7914 - val_loss: 0.1741 - val_coeff_deter: 0.9374\nEpoch 25/50\n - 0s - loss: 0.2897 - coeff_deter: 0.8048 - val_loss: 0.1834 - val_coeff_deter: 0.9231\nThere are 4 cointegrated stocks.\nTrain on 3311 samples, validate on 2208 samples\nEpoch 1/50\n - 1s - loss: 0.4104 - coeff_deter: 0.6309 - val_loss: 0.2991 - val_coeff_deter: 0.8341\nEpoch 2/50\n - 1s - loss: 0.3418 - coeff_deter: 0.7489 - val_loss: 0.2834 - val_coeff_deter: 0.8508\nEpoch 3/50\n - 1s - loss: 0.3424 - coeff_deter: 0.7444 - val_loss: 0.2610 - val_coeff_deter: 0.8716\nEpoch 4/50\n - 1s - loss: 0.3246 - coeff_deter: 0.7715 - val_loss: 0.2401 - val_coeff_deter: 0.8943\nEpoch 5/50\n - 1s - loss: 0.3203 - coeff_deter: 0.7768 - val_loss: 0.2232 - val_coeff_deter: 0.8995\nEpoch 6/50\n - 1s - loss: 0.3190 - coeff_deter: 0.7857 - val_loss: 0.2538 - val_coeff_deter: 0.8806\nEpoch 7/50\n - 1s - loss: 0.3182 - coeff_deter: 0.7817 - val_loss: 0.2406 - val_coeff_deter: 0.8958\nEpoch 8/50\n - 1s - loss: 0.3135 - coeff_deter: 0.7870 - val_loss: 0.2381 - val_coeff_deter: 0.8959\nEpoch 9/50\n - 1s - loss: 0.3102 - coeff_deter: 0.7915 - val_loss: 0.2285 - val_coeff_deter: 0.9020\nEpoch 10/50\n - 1s - loss: 0.3071 - coeff_deter: 0.7964 - val_loss: 0.2461 - val_coeff_deter: 0.8894\nEpoch 11/50\n - 1s - loss: 0.3008 - coeff_deter: 0.8031 - val_loss: 0.2424 - val_coeff_deter: 0.8951\nEpoch 12/50\n - 1s - loss: 0.3077 - coeff_deter: 0.7922 - val_loss: 0.2135 - val_coeff_deter: 0.9153\nEpoch 13/50\n - 1s - loss: 0.3044 - coeff_deter: 0.7885 - val_loss: 0.2123 - val_coeff_deter: 0.9160\nEpoch 14/50\n - 1s - loss: 0.3053 - coeff_deter: 0.8022 - val_loss: 0.2039 - val_coeff_deter: 0.9218\nEpoch 15/50\n - 1s - loss: 0.3045 - coeff_deter: 0.8010 - val_loss: 0.2261 - val_coeff_deter: 0.9067\nEpoch 16/50\n - 1s - loss: 0.3033 - coeff_deter: 0.8016 - val_loss: 0.2015 - val_coeff_deter: 0.9208\nEpoch 17/50\n - 1s - loss: 0.3091 - coeff_deter: 0.7865 - val_loss: 0.2139 - val_coeff_deter: 0.9142\nEpoch 18/50\n - 1s - loss: 0.3037 - coeff_deter: 0.7959 - val_loss: 0.2650 - val_coeff_deter: 0.8676\nEpoch 19/50\n - 1s - loss: 0.3047 - coeff_deter: 0.7999 - val_loss: 0.2165 - val_coeff_deter: 0.9070\nThere are 3 cointegrated stocks.\nTrain on 3311 samples, validate on 2208 samples\nEpoch 1/50\n - 1s - loss: 0.3897 - coeff_deter: 0.6575 - val_loss: 0.3114 - val_coeff_deter: 0.8113\nEpoch 2/50\n - 1s - loss: 0.3519 - coeff_deter: 0.7246 - val_loss: 0.2699 - val_coeff_deter: 0.8632\nEpoch 3/50\n - 1s - loss: 0.3378 - coeff_deter: 0.7495 - val_loss: 0.2839 - val_coeff_deter: 0.8498\nEpoch 4/50\n - 1s - loss: 0.3293 - coeff_deter: 0.7644 - val_loss: 0.2713 - val_coeff_deter: 0.8630\nEpoch 5/50\n - 1s - loss: 0.3263 - coeff_deter: 0.7432 - val_loss: 0.2825 - val_coeff_deter: 0.8453\nEpoch 6/50\n - 1s - loss: 0.3132 - coeff_deter: 0.7857 - val_loss: 0.2438 - val_coeff_deter: 0.8823\nEpoch 7/50\n - 1s - loss: 0.3275 - coeff_deter: 0.7547 - val_loss: 0.2685 - val_coeff_deter: 0.8620\nEpoch 8/50\n - 1s - loss: 0.3107 - coeff_deter: 0.7836 - val_loss: 0.2562 - val_coeff_deter: 0.8759\nEpoch 9/50\n - 1s - loss: 0.3147 - coeff_deter: 0.7773 - val_loss: 0.2409 - val_coeff_deter: 0.8880\nEpoch 10/50\n - 1s - loss: 0.3169 - coeff_deter: 0.7659 - val_loss: 0.2558 - val_coeff_deter: 0.8635\nEpoch 11/50\n - 1s - loss: 0.3122 - coeff_deter: 0.7798 - val_loss: 0.2343 - val_coeff_deter: 0.8890\nEpoch 12/50\n - 1s - loss: 0.3111 - coeff_deter: 0.7781 - val_loss: 0.2470 - val_coeff_deter: 0.8808\nEpoch 13/50\n - 1s - loss: 0.3053 - coeff_deter: 0.7872 - val_loss: 0.2341 - val_coeff_deter: 0.8922\nEpoch 14/50\n - 1s - loss: 0.3046 - coeff_deter: 0.7897 - val_loss: 0.2704 - val_coeff_deter: 0.8600\nEpoch 15/50\n - 1s - loss: 0.3022 - coeff_deter: 0.7898 - val_loss: 0.2379 - val_coeff_deter: 0.8877\nEpoch 16/50\n - 1s - loss: 0.3061 - coeff_deter: 0.7836 - val_loss: 0.2645 - val_coeff_deter: 0.8616\nEpoch 17/50\n - 1s - loss: 0.3111 - coeff_deter: 0.7827 - val_loss: 0.2537 - val_coeff_deter: 0.8774\nEpoch 18/50\n - 1s - loss: 0.3026 - coeff_deter: 0.7824 - val_loss: 0.2546 - val_coeff_deter: 0.8755\nThere are 3 cointegrated stocks.\nTrain on 3311 samples, validate on 2208 samples\nEpoch 1/50\n - 1s - loss: 0.3967 - coeff_deter: 0.6256 - val_loss: 0.2878 - val_coeff_deter: 0.8268\nEpoch 2/50\n - 1s - loss: 0.3554 - coeff_deter: 0.7034 - val_loss: 0.2828 - val_coeff_deter: 0.8308\nEpoch 3/50\n - 1s - loss: 0.3386 - coeff_deter: 0.7309 - val_loss: 0.3068 - val_coeff_deter: 0.7936\nEpoch 4/50\n - 1s - loss: 0.3299 - coeff_deter: 0.7413 - val_loss: 0.2389 - val_coeff_deter: 0.8738\nEpoch 5/50\n - 1s - loss: 0.3335 - coeff_deter: 0.7281 - val_loss: 0.3182 - val_coeff_deter: 0.7882\nEpoch 6/50\n - 1s - loss: 0.3244 - coeff_deter: 0.7589 - val_loss: 0.2765 - val_coeff_deter: 0.8232\nEpoch 7/50\n - 0s - loss: 0.3207 - coeff_deter: 0.7374 - val_loss: 0.2583 - val_coeff_deter: 0.8623\nEpoch 8/50\n - 1s - loss: 0.3335 - coeff_deter: 0.7247 - val_loss: 0.2431 - val_coeff_deter: 0.8757\nEpoch 9/50\n - 1s - loss: 0.3210 - coeff_deter: 0.7478 - val_loss: 0.2892 - val_coeff_deter: 0.8263\nEpoch 10/50\n - 1s - loss: 0.3217 - coeff_deter: 0.7452 - val_loss: 0.2554 - val_coeff_deter: 0.8648\nEpoch 11/50\n - 1s - loss: 0.3217 - coeff_deter: 0.7412 - val_loss: 0.2303 - val_coeff_deter: 0.8752\nEpoch 12/50\n - 1s - loss: 0.3211 - coeff_deter: 0.7449 - val_loss: 0.2430 - val_coeff_deter: 0.8767\nEpoch 13/50\n - 0s - loss: 0.3164 - coeff_deter: 0.7482 - val_loss: 0.2384 - val_coeff_deter: 0.8787\nEpoch 14/50\n - 1s - loss: 0.3155 - coeff_deter: 0.7667 - val_loss: 0.2634 - val_coeff_deter: 0.8526\nEpoch 15/50\n - 0s - loss: 0.3145 - coeff_deter: 0.7603 - val_loss: 0.2231 - val_coeff_deter: 0.8860\nEpoch 16/50\n - 1s - loss: 0.3103 - coeff_deter: 0.7621 - val_loss: 0.2439 - val_coeff_deter: 0.8732\nEpoch 17/50\n - 1s - loss: 0.3116 - coeff_deter: 0.7613 - val_loss: 0.2328 - val_coeff_deter: 0.8721\nEpoch 18/50\n - 1s - loss: 0.3145 - coeff_deter: 0.7624 - val_loss: 0.2349 - val_coeff_deter: 0.8731\nEpoch 19/50\n - 1s - loss: 0.3031 - coeff_deter: 0.7698 - val_loss: 0.2318 - val_coeff_deter: 0.8747\nEpoch 20/50\n - 1s - loss: 0.3128 - coeff_deter: 0.7613 - val_loss: 0.2565 - val_coeff_deter: 0.8655\nThere are 2 cointegrated stocks.\nTrain on 3311 samples, validate on 2208 samples\nEpoch 1/50\n - 1s - loss: 0.4104 - coeff_deter: 0.6012 - val_loss: 0.2868 - val_coeff_deter: 0.7998\nEpoch 2/50\n - 1s - loss: 0.3284 - coeff_deter: 0.7242 - val_loss: 0.2352 - val_coeff_deter: 0.8781\nEpoch 3/50\n - 1s - loss: 0.3211 - coeff_deter: 0.7481 - val_loss: 0.2333 - val_coeff_deter: 0.8800\nEpoch 4/50\n - 1s - loss: 0.3110 - coeff_deter: 0.7653 - val_loss: 0.2652 - val_coeff_deter: 0.8507\nEpoch 5/50\n - 1s - loss: 0.3060 - coeff_deter: 0.7763 - val_loss: 0.2215 - val_coeff_deter: 0.8892\nEpoch 6/50\n - 1s - loss: 0.3064 - coeff_deter: 0.7681 - val_loss: 0.2394 - val_coeff_deter: 0.8804\nEpoch 7/50\n - 1s - loss: 0.3067 - coeff_deter: 0.7666 - val_loss: 0.2185 - val_coeff_deter: 0.8918\nEpoch 8/50\n - 1s - loss: 0.3043 - coeff_deter: 0.7734 - val_loss: 0.2175 - val_coeff_deter: 0.8927\nEpoch 9/50\n - 1s - loss: 0.3028 - coeff_deter: 0.7760 - val_loss: 0.2378 - val_coeff_deter: 0.8749\nEpoch 10/50\n - 1s - loss: 0.2950 - coeff_deter: 0.7811 - val_loss: 0.2194 - val_coeff_deter: 0.8937\nEpoch 11/50\n - 1s - loss: 0.2988 - coeff_deter: 0.7823 - val_loss: 0.2437 - val_coeff_deter: 0.8710\nEpoch 12/50\n - 1s - loss: 0.2988 - coeff_deter: 0.7760 - val_loss: 0.2269 - val_coeff_deter: 0.8845\nEpoch 13/50\n - 1s - loss: 0.2986 - coeff_deter: 0.7777 - val_loss: 0.2501 - val_coeff_deter: 0.8608\nEpoch 14/50\n - 1s - loss: 0.3002 - coeff_deter: 0.7816 - val_loss: 0.2186 - val_coeff_deter: 0.8946\nEpoch 15/50\n - 1s - loss: 0.2985 - coeff_deter: 0.7745 - val_loss: 0.2166 - val_coeff_deter: 0.8978\nEpoch 16/50\n - 1s - loss: 0.2904 - coeff_deter: 0.7766 - val_loss: 0.2557 - val_coeff_deter: 0.8509\nEpoch 17/50\n - 1s - loss: 0.2926 - coeff_deter: 0.7827 - val_loss: 0.2099 - val_coeff_deter: 0.8997\nEpoch 18/50\n - 1s - loss: 0.2913 - coeff_deter: 0.7958 - val_loss: 0.2164 - val_coeff_deter: 0.8900\nEpoch 19/50\n - 1s - loss: 0.2937 - coeff_deter: 0.7861 - val_loss: 0.2191 - val_coeff_deter: 0.8913\nEpoch 20/50\n - 1s - loss: 0.2874 - coeff_deter: 0.7886 - val_loss: 0.2072 - val_coeff_deter: 0.9016\nEpoch 21/50\n - 1s - loss: 0.2826 - coeff_deter: 0.7958 - val_loss: 0.2201 - val_coeff_deter: 0.8931\nEpoch 22/50\n - 1s - loss: 0.2898 - coeff_deter: 0.7905 - val_loss: 0.2593 - val_coeff_deter: 0.8468\nEpoch 23/50\n - 1s - loss: 0.2863 - coeff_deter: 0.7846 - val_loss: 0.1919 - val_coeff_deter: 0.9124\nEpoch 24/50\n - 1s - loss: 0.2913 - coeff_deter: 0.7882 - val_loss: 0.1907 - val_coeff_deter: 0.9149\nEpoch 25/50\n - 1s - loss: 0.2875 - coeff_deter: 0.7858 - val_loss: 0.1978 - val_coeff_deter: 0.9090\nEpoch 26/50\n - 1s - loss: 0.2893 - coeff_deter: 0.7833 - val_loss: 0.2276 - val_coeff_deter: 0.8845\nEpoch 27/50\n - 1s - loss: 0.2904 - coeff_deter: 0.7842 - val_loss: 0.2381 - val_coeff_deter: 0.8745\nEpoch 28/50\n - 1s - loss: 0.2838 - coeff_deter: 0.7984 - val_loss: 0.2314 - val_coeff_deter: 0.8796\nEpoch 29/50\n - 1s - loss: 0.2851 - coeff_deter: 0.7945 - val_loss: 0.2171 - val_coeff_deter: 0.8941\nThere are 2 cointegrated stocks.\nTrain on 3311 samples, validate on 2208 samples\nEpoch 1/50\n - 1s - loss: 0.3793 - coeff_deter: 0.6148 - val_loss: 0.2403 - val_coeff_deter: 0.8433\nEpoch 2/50\n - 1s - loss: 0.2962 - coeff_deter: 0.7702 - val_loss: 0.2036 - val_coeff_deter: 0.8919\nEpoch 3/50\n - 1s - loss: 0.2929 - coeff_deter: 0.7664 - val_loss: 0.2078 - val_coeff_deter: 0.8887\nEpoch 4/50\n - 1s - loss: 0.2942 - coeff_deter: 0.7626 - val_loss: 0.2454 - val_coeff_deter: 0.8512\nEpoch 5/50\n - 1s - loss: 0.2807 - coeff_deter: 0.7853 - val_loss: 0.1854 - val_coeff_deter: 0.9145\nEpoch 6/50\n - 1s - loss: 0.2868 - coeff_deter: 0.7716 - val_loss: 0.2039 - val_coeff_deter: 0.8977\nEpoch 7/50\n - 1s - loss: 0.2840 - coeff_deter: 0.7683 - val_loss: 0.1951 - val_coeff_deter: 0.9047\nEpoch 8/50\n - 1s - loss: 0.2746 - coeff_deter: 0.7884 - val_loss: 0.1892 - val_coeff_deter: 0.9105\nEpoch 9/50\n - 1s - loss: 0.2782 - coeff_deter: 0.7824 - val_loss: 0.2001 - val_coeff_deter: 0.8993\nEpoch 10/50\n - 1s - loss: 0.2741 - coeff_deter: 0.7852 - val_loss: 0.1924 - val_coeff_deter: 0.9029\nThere are 2 cointegrated stocks.\nTrain on 3311 samples, validate on 2208 samples\nEpoch 1/50\n - 1s - loss: 0.3443 - coeff_deter: 0.6369 - val_loss: 0.1946 - val_coeff_deter: 0.9005\nEpoch 2/50\n - 1s - loss: 0.2740 - coeff_deter: 0.7808 - val_loss: 0.1728 - val_coeff_deter: 0.9210\nEpoch 3/50\n - 1s - loss: 0.2609 - coeff_deter: 0.7964 - val_loss: 0.1771 - val_coeff_deter: 0.9217\nEpoch 4/50\n - 1s - loss: 0.2541 - coeff_deter: 0.8103 - val_loss: 0.1593 - val_coeff_deter: 0.9361\nEpoch 5/50\n - 1s - loss: 0.2476 - coeff_deter: 0.8244 - val_loss: 0.1578 - val_coeff_deter: 0.9325\nEpoch 6/50\n - 1s - loss: 0.2487 - coeff_deter: 0.8220 - val_loss: 0.1946 - val_coeff_deter: 0.9043\nEpoch 7/50\n - 1s - loss: 0.2413 - coeff_deter: 0.8223 - val_loss: 0.1751 - val_coeff_deter: 0.9180\nEpoch 8/50\n - 1s - loss: 0.2461 - coeff_deter: 0.8166 - val_loss: 0.1885 - val_coeff_deter: 0.9069\nEpoch 9/50\n - 1s - loss: 0.2391 - coeff_deter: 0.8274 - val_loss: 0.1584 - val_coeff_deter: 0.9305\nThere are 2 cointegrated stocks.\nTrain on 3311 samples, validate on 2208 samples\nEpoch 1/50\n - 1s - loss: 0.3335 - coeff_deter: 0.6487 - val_loss: 0.1708 - val_coeff_deter: 0.9253\nEpoch 2/50\n - 1s - loss: 0.2603 - coeff_deter: 0.7986 - val_loss: 0.1820 - val_coeff_deter: 0.9094\nEpoch 3/50\n - 1s - loss: 0.2468 - coeff_deter: 0.8059 - val_loss: 0.1480 - val_coeff_deter: 0.9416\nEpoch 4/50\n - 1s - loss: 0.2366 - coeff_deter: 0.8303 - val_loss: 0.1421 - val_coeff_deter: 0.9491\nEpoch 5/50\n - 1s - loss: 0.2396 - coeff_deter: 0.8295 - val_loss: 0.1683 - val_coeff_deter: 0.9243\nEpoch 6/50\n - 1s - loss: 0.2279 - coeff_deter: 0.8280 - val_loss: 0.1395 - val_coeff_deter: 0.9497\nEpoch 7/50\n - 1s - loss: 0.2330 - coeff_deter: 0.8201 - val_loss: 0.1636 - val_coeff_deter: 0.9256\nEpoch 8/50\n - 1s - loss: 0.2317 - coeff_deter: 0.8400 - val_loss: 0.1691 - val_coeff_deter: 0.9235\nEpoch 9/50\n - 1s - loss: 0.2272 - coeff_deter: 0.8348 - val_loss: 0.1382 - val_coeff_deter: 0.9517\nEpoch 10/50\n - 1s - loss: 0.2272 - coeff_deter: 0.8337 - val_loss: 0.1481 - val_coeff_deter: 0.9469\nEpoch 11/50\n - 1s - loss: 0.2285 - coeff_deter: 0.8342 - val_loss: 0.1339 - val_coeff_deter: 0.9505\nEpoch 12/50\n - 1s - loss: 0.2232 - coeff_deter: 0.8359 - val_loss: 0.1696 - val_coeff_deter: 0.9284\nEpoch 13/50\n - 1s - loss: 0.2178 - coeff_deter: 0.8494 - val_loss: 0.1337 - val_coeff_deter: 0.9548\nEpoch 14/50\n - 1s - loss: 0.2255 - coeff_deter: 0.8413 - val_loss: 0.1705 - val_coeff_deter: 0.9228\nEpoch 15/50\n - 1s - loss: 0.2206 - coeff_deter: 0.8460 - val_loss: 0.1326 - val_coeff_deter: 0.9524\nEpoch 16/50\n - 1s - loss: 0.2169 - coeff_deter: 0.8417 - val_loss: 0.1444 - val_coeff_deter: 0.9437\nEpoch 17/50\n - 1s - loss: 0.2198 - coeff_deter: 0.8492 - val_loss: 0.1323 - val_coeff_deter: 0.9537\nEpoch 18/50\n - 1s - loss: 0.2242 - coeff_deter: 0.8362 - val_loss: 0.1553 - val_coeff_deter: 0.9371\nThere are 3 cointegrated stocks.\nTrain on 3311 samples, validate on 2208 samples\nEpoch 1/50\n - 1s - loss: 0.3109 - coeff_deter: 0.6932 - val_loss: 0.1867 - val_coeff_deter: 0.8973\nEpoch 2/50\n - 1s - loss: 0.2495 - coeff_deter: 0.7982 - val_loss: 0.1417 - val_coeff_deter: 0.9382\nEpoch 3/50\n - 1s - loss: 0.2449 - coeff_deter: 0.8051 - val_loss: 0.1503 - val_coeff_deter: 0.9325\nEpoch 4/50\n - 1s - loss: 0.2354 - coeff_deter: 0.8055 - val_loss: 0.1441 - val_coeff_deter: 0.9394\nEpoch 5/50\n - 1s - loss: 0.2294 - coeff_deter: 0.8270 - val_loss: 0.1431 - val_coeff_deter: 0.9394\nEpoch 6/50\n - 1s - loss: 0.2207 - coeff_deter: 0.8364 - val_loss: 0.1373 - val_coeff_deter: 0.9461\nEpoch 7/50\n - 1s - loss: 0.2263 - coeff_deter: 0.8262 - val_loss: 0.1617 - val_coeff_deter: 0.9186\nEpoch 8/50\n - 1s - loss: 0.2160 - coeff_deter: 0.8454 - val_loss: 0.1274 - val_coeff_deter: 0.9515\nEpoch 9/50\n - 1s - loss: 0.2169 - coeff_deter: 0.8335 - val_loss: 0.1305 - val_coeff_deter: 0.9534\nEpoch 10/50\n - 1s - loss: 0.2149 - coeff_deter: 0.8344 - val_loss: 0.1287 - val_coeff_deter: 0.9495\nEpoch 11/50\n - 1s - loss: 0.2152 - coeff_deter: 0.8347 - val_loss: 0.1648 - val_coeff_deter: 0.9228\nEpoch 12/50\n - 1s - loss: 0.2194 - coeff_deter: 0.8206 - val_loss: 0.1249 - val_coeff_deter: 0.9510\nEpoch 13/50\n - 1s - loss: 0.2081 - coeff_deter: 0.8390 - val_loss: 0.1365 - val_coeff_deter: 0.9388\nEpoch 14/50\n - 1s - loss: 0.2152 - coeff_deter: 0.8271 - val_loss: 0.1228 - val_coeff_deter: 0.9562\nEpoch 15/50\n - 1s - loss: 0.2150 - coeff_deter: 0.8316 - val_loss: 0.1609 - val_coeff_deter: 0.9186\nEpoch 16/50\n - 1s - loss: 0.2147 - coeff_deter: 0.8310 - val_loss: 0.1311 - val_coeff_deter: 0.9474\nEpoch 17/50\n - 1s - loss: 0.2065 - coeff_deter: 0.8491 - val_loss: 0.1557 - val_coeff_deter: 0.9211\nEpoch 18/50\n - 1s - loss: 0.2095 - coeff_deter: 0.8303 - val_loss: 0.1309 - val_coeff_deter: 0.9483\nEpoch 19/50\n - 1s - loss: 0.2119 - coeff_deter: 0.8261 - val_loss: 0.1306 - val_coeff_deter: 0.9512\nThere are 2 cointegrated stocks.\nTrain on 3311 samples, validate on 2208 samples\nEpoch 1/50\n - 1s - loss: 0.3070 - coeff_deter: 0.6727 - val_loss: 0.1594 - val_coeff_deter: 0.9225\nEpoch 2/50\n - 1s - loss: 0.2374 - coeff_deter: 0.8142 - val_loss: 0.1518 - val_coeff_deter: 0.9275\nEpoch 3/50\n - 1s - loss: 0.2274 - coeff_deter: 0.8250 - val_loss: 0.1544 - val_coeff_deter: 0.9345\nEpoch 4/50\n - 1s - loss: 0.2216 - coeff_deter: 0.8322 - val_loss: 0.1272 - val_coeff_deter: 0.9523\nEpoch 5/50\n - 1s - loss: 0.2169 - coeff_deter: 0.8372 - val_loss: 0.1644 - val_coeff_deter: 0.9201\nEpoch 6/50\n - 1s - loss: 0.2095 - coeff_deter: 0.8327 - val_loss: 0.1335 - val_coeff_deter: 0.9444\nEpoch 7/50\n - 1s - loss: 0.2058 - coeff_deter: 0.8489 - val_loss: 0.1616 - val_coeff_deter: 0.9042\nEpoch 8/50\n - 1s - loss: 0.2080 - coeff_deter: 0.8470 - val_loss: 0.1294 - val_coeff_deter: 0.9533\nEpoch 9/50\n - 1s - loss: 0.2040 - coeff_deter: 0.8483 - val_loss: 0.1342 - val_coeff_deter: 0.9365\nEpoch 10/50\n - 1s - loss: 0.2072 - coeff_deter: 0.8362 - val_loss: 0.1552 - val_coeff_deter: 0.9236\nEpoch 11/50\n - 1s - loss: 0.1975 - coeff_deter: 0.8423 - val_loss: 0.1233 - val_coeff_deter: 0.9542\nEpoch 12/50\n - 1s - loss: 0.1972 - coeff_deter: 0.8509 - val_loss: 0.1886 - val_coeff_deter: 0.8906\nEpoch 13/50\n - 1s - loss: 0.2007 - coeff_deter: 0.8519 - val_loss: 0.1191 - val_coeff_deter: 0.9579\nEpoch 14/50\n - 1s - loss: 0.1973 - coeff_deter: 0.8338 - val_loss: 0.1171 - val_coeff_deter: 0.9592\nEpoch 15/50\n - 1s - loss: 0.2011 - coeff_deter: 0.8363 - val_loss: 0.1156 - val_coeff_deter: 0.9598\nEpoch 16/50\n - 1s - loss: 0.1960 - coeff_deter: 0.8555 - val_loss: 0.1180 - val_coeff_deter: 0.9601\nEpoch 17/50\n - 1s - loss: 0.1975 - coeff_deter: 0.8435 - val_loss: 0.1192 - val_coeff_deter: 0.9546\nEpoch 18/50\n - 1s - loss: 0.1942 - coeff_deter: 0.8483 - val_loss: 0.1768 - val_coeff_deter: 0.8933\nEpoch 19/50\n - 1s - loss: 0.1934 - coeff_deter: 0.8516 - val_loss: 0.1261 - val_coeff_deter: 0.9498\nEpoch 20/50\n - 1s - loss: 0.1949 - coeff_deter: 0.8493 - val_loss: 0.1419 - val_coeff_deter: 0.9382\nEpoch 21/50\n - 1s - loss: 0.1992 - coeff_deter: 0.8367 - val_loss: 0.1179 - val_coeff_deter: 0.9604\nEpoch 22/50\n - 1s - loss: 0.1924 - coeff_deter: 0.8551 - val_loss: 0.1501 - val_coeff_deter: 0.9241\nEpoch 23/50\n - 1s - loss: 0.1948 - coeff_deter: 0.8451 - val_loss: 0.1313 - val_coeff_deter: 0.9431\nEpoch 24/50\n - 1s - loss: 0.1964 - coeff_deter: 0.8396 - val_loss: 0.1308 - val_coeff_deter: 0.9471\nEpoch 25/50\n - 1s - loss: 0.2025 - coeff_deter: 0.8320 - val_loss: 0.1173 - val_coeff_deter: 0.9563\nEpoch 26/50\n - 1s - loss: 0.1955 - coeff_deter: 0.8486 - val_loss: 0.1335 - val_coeff_deter: 0.9414\n"
    }
   ],
   "source": [
    "for i in range(T):\n",
    "\n",
    "    test_X = []\n",
    "\n",
    "    for t in range(120):\n",
    "        test_prep = original_data[coint_corr].iloc[(cutoff-30+t+i*120):(cutoff+t+i*120)].values.T.flatten()\n",
    "        test_X.append(test_prep)\n",
    "\n",
    "    test_X = np.array(test_X)\n",
    "    scale_test_X = mm_scaler_x.transform(test_X)\n",
    "    scale_test_X = scale_test_X.reshape(scale_test_X.shape[0], 1, scale_test_X.shape[1])\n",
    "\n",
    "    y_pred_os = lstm_mod.predict(scale_test_X)\n",
    "    y_pred_os = mm_scaler_y.inverse_transform(y_pred_os)\n",
    "\n",
    "    init_asset, record = measure_profit(tick, y_pred_os, init_asset, test_data.iloc[i * 120:(i + 1) * 120])\n",
    "    mlrecord_os += record\n",
    "\n",
    "    # update model after record performance\n",
    "    new_observed_data = original_data.iloc[i * 120:cutoff + (i + 1) * 120]\n",
    "    coint_corr = coint_group(tick, new_observed_data)\n",
    "\n",
    "    X, Y = [], []\n",
    "\n",
    "    for t in range(len(new_observed_data) - train_length):\n",
    "        x = new_observed_data[coint_corr].iloc[t:t + train_length].values.T.flatten()\n",
    "        y = new_observed_data['%s_LAG' % tick].iloc[t + train_length]\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y).reshape(-1, 1)\n",
    "\n",
    "    mm_scaler_x = StandardScaler()\n",
    "    mm_scaler_y = StandardScaler()\n",
    "    lstm_mod = LSTM_mod(X, Y, mm_scaler_x, mm_scaler_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_X = []\n",
    "\n",
    "for t in range(test_data.shape[0] % 120):\n",
    "    test_prep = original_data[coint_corr].iloc[(cutoff-30+t+T*120):(cutoff+t+T*120)].values.T.flatten()\n",
    "    test_X.append(test_prep)\n",
    "\n",
    "test_X = np.array(test_X)\n",
    "scale_test_X = mm_scaler_x.transform(test_X)\n",
    "scale_test_X = scale_test_X.reshape(scale_test_X.shape[0], 1, scale_test_X.shape[1])\n",
    "\n",
    "y_pred_os = lstm_mod.predict(scale_test_X)\n",
    "y_pred_os = mm_scaler_y.inverse_transform(y_pred_os)\n",
    "\n",
    "init_asset, record = measure_profit(tick, y_pred_os, init_asset, test_data.iloc[T * 120:])\n",
    "mlrecord_os += record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "2.739999999999995"
     },
     "metadata": {},
     "execution_count": 157
    }
   ],
   "source": [
    "init_asset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.array([1,5,10,15, 100, 50, 100, 25, 86, 90])\n",
    "test_scaler = MinMaxScaler()\n",
    "\n",
    "def test_func(arr, scaler):\n",
    "    scaler = scaler.fit(arr.reshape(-1,1))\n",
    "    return scaler.transform(arr.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0.        ],\n       [0.04040404],\n       [0.09090909],\n       [0.14141414],\n       [1.        ],\n       [0.49494949],\n       [1.        ],\n       [0.24242424],\n       [0.85858586],\n       [0.8989899 ]])"
     },
     "metadata": {},
     "execution_count": 114
    }
   ],
   "source": [
    "test_func(test, test_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0.        ],\n       [0.04040404],\n       [0.09090909],\n       [0.14141414],\n       [1.        ],\n       [0.49494949],\n       [1.        ],\n       [0.24242424],\n       [0.85858586],\n       [0.8989899 ]])"
     },
     "metadata": {},
     "execution_count": 115
    }
   ],
   "source": [
    "test_scaler.transform(test.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.read_csv('Regression_Prediction_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.drop(['Unnamed: 0'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.columns = ['in_profit', 'in_var', 'os_profit', 'os_var']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['os/in profit'] = result['os_profit'] / result['in_profit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "      in_profit     in_var  os_profit      os_var  os/in profit\nZBH       19.20   0.248987      51.06    4.243600      2.659375\nCLGX      39.04   0.023419      35.54    0.468477      0.910348\nROST      34.29   0.006816      30.41    0.938737      0.886847\nGGG       20.77   0.002549      15.32    0.287751      0.737602\nCBU       49.19   0.021785      36.23    0.414882      0.736532\nAPD      154.04   0.196917     111.95    4.757181      0.726759\nETR       64.19   0.081010      45.94    1.114922      0.715688\nPRFT      23.82   0.009355      16.39    0.145097      0.688077\nADBE     134.69   0.199759      92.42    3.827720      0.686168\nHPQ       22.31   0.007270      13.86    0.054593      0.621246\nPATK      21.82   0.005070      13.46    0.339227      0.616865\nPSB       82.71   0.072147      45.46    2.409283      0.549631\nIRBT      65.77   0.062752      35.70    2.334672      0.542801\nNSP       37.91   0.020396      20.51    0.857575      0.541018\nGOOG    1072.67  11.181631     510.25  327.525902      0.475682\nATNI      59.43   0.021928      26.78    0.775694      0.450614\nGWW      256.71   0.282923     113.74   10.539124      0.443068\nRYN       20.95   0.007006       8.83    0.155329      0.421480\nOXM       81.29   0.121578      33.29    0.787492      0.409521\nKR        31.02   0.005382      11.86    0.168161      0.382334",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>in_profit</th>\n      <th>in_var</th>\n      <th>os_profit</th>\n      <th>os_var</th>\n      <th>os/in profit</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>ZBH</th>\n      <td>19.20</td>\n      <td>0.248987</td>\n      <td>51.06</td>\n      <td>4.243600</td>\n      <td>2.659375</td>\n    </tr>\n    <tr>\n      <th>CLGX</th>\n      <td>39.04</td>\n      <td>0.023419</td>\n      <td>35.54</td>\n      <td>0.468477</td>\n      <td>0.910348</td>\n    </tr>\n    <tr>\n      <th>ROST</th>\n      <td>34.29</td>\n      <td>0.006816</td>\n      <td>30.41</td>\n      <td>0.938737</td>\n      <td>0.886847</td>\n    </tr>\n    <tr>\n      <th>GGG</th>\n      <td>20.77</td>\n      <td>0.002549</td>\n      <td>15.32</td>\n      <td>0.287751</td>\n      <td>0.737602</td>\n    </tr>\n    <tr>\n      <th>CBU</th>\n      <td>49.19</td>\n      <td>0.021785</td>\n      <td>36.23</td>\n      <td>0.414882</td>\n      <td>0.736532</td>\n    </tr>\n    <tr>\n      <th>APD</th>\n      <td>154.04</td>\n      <td>0.196917</td>\n      <td>111.95</td>\n      <td>4.757181</td>\n      <td>0.726759</td>\n    </tr>\n    <tr>\n      <th>ETR</th>\n      <td>64.19</td>\n      <td>0.081010</td>\n      <td>45.94</td>\n      <td>1.114922</td>\n      <td>0.715688</td>\n    </tr>\n    <tr>\n      <th>PRFT</th>\n      <td>23.82</td>\n      <td>0.009355</td>\n      <td>16.39</td>\n      <td>0.145097</td>\n      <td>0.688077</td>\n    </tr>\n    <tr>\n      <th>ADBE</th>\n      <td>134.69</td>\n      <td>0.199759</td>\n      <td>92.42</td>\n      <td>3.827720</td>\n      <td>0.686168</td>\n    </tr>\n    <tr>\n      <th>HPQ</th>\n      <td>22.31</td>\n      <td>0.007270</td>\n      <td>13.86</td>\n      <td>0.054593</td>\n      <td>0.621246</td>\n    </tr>\n    <tr>\n      <th>PATK</th>\n      <td>21.82</td>\n      <td>0.005070</td>\n      <td>13.46</td>\n      <td>0.339227</td>\n      <td>0.616865</td>\n    </tr>\n    <tr>\n      <th>PSB</th>\n      <td>82.71</td>\n      <td>0.072147</td>\n      <td>45.46</td>\n      <td>2.409283</td>\n      <td>0.549631</td>\n    </tr>\n    <tr>\n      <th>IRBT</th>\n      <td>65.77</td>\n      <td>0.062752</td>\n      <td>35.70</td>\n      <td>2.334672</td>\n      <td>0.542801</td>\n    </tr>\n    <tr>\n      <th>NSP</th>\n      <td>37.91</td>\n      <td>0.020396</td>\n      <td>20.51</td>\n      <td>0.857575</td>\n      <td>0.541018</td>\n    </tr>\n    <tr>\n      <th>GOOG</th>\n      <td>1072.67</td>\n      <td>11.181631</td>\n      <td>510.25</td>\n      <td>327.525902</td>\n      <td>0.475682</td>\n    </tr>\n    <tr>\n      <th>ATNI</th>\n      <td>59.43</td>\n      <td>0.021928</td>\n      <td>26.78</td>\n      <td>0.775694</td>\n      <td>0.450614</td>\n    </tr>\n    <tr>\n      <th>GWW</th>\n      <td>256.71</td>\n      <td>0.282923</td>\n      <td>113.74</td>\n      <td>10.539124</td>\n      <td>0.443068</td>\n    </tr>\n    <tr>\n      <th>RYN</th>\n      <td>20.95</td>\n      <td>0.007006</td>\n      <td>8.83</td>\n      <td>0.155329</td>\n      <td>0.421480</td>\n    </tr>\n    <tr>\n      <th>OXM</th>\n      <td>81.29</td>\n      <td>0.121578</td>\n      <td>33.29</td>\n      <td>0.787492</td>\n      <td>0.409521</td>\n    </tr>\n    <tr>\n      <th>KR</th>\n      <td>31.02</td>\n      <td>0.005382</td>\n      <td>11.86</td>\n      <td>0.168161</td>\n      <td>0.382334</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "result.sort_values(['os/in profit'], ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}