{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yfinance as yf\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import pytz\n",
    "import logging\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torch.optim import Adam, SGD\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dta_to_candlestick(data):\n",
    "    l = len(data)\n",
    "    # Make candlestick picture\n",
    "    layout = go.Layout(xaxis=dict(ticks='',\n",
    "                                  showgrid=False,\n",
    "                                  showticklabels=False,\n",
    "                                  rangeslider=dict(visible=False)),\n",
    "                       yaxis=dict(ticks='',\n",
    "                                  showgrid=False,\n",
    "                                  showticklabels=False),\n",
    "                       width=300,\n",
    "                       height=300,\n",
    "                       paper_bgcolor='rgba(0,0,0,0)',\n",
    "                       plot_bgcolor='rgba(0,0,0,0)')\n",
    "    fig = go.Figure(data=[go.Candlestick(x=np.linspace(1,l,l),\n",
    "                                         open=data.Open,\n",
    "                                         high=data.High,\n",
    "                                         low=data.Low,\n",
    "                                         close=data.Close)],\n",
    "                    layout=layout)\n",
    "    fig.write_image(\"images/fig-33.png\")\n",
    "\n",
    "    # Convert to numpy array\n",
    "    im = Image.open('images/fig-33.png')\n",
    "    #im = im.resize((300,300),Image.ANTIALIAS)\n",
    "    data = np.asarray(im)\n",
    "\n",
    "    # Return the first channel of the image\n",
    "    return data[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "START = datetime(1980, 1, 1)\n",
    "END = datetime(2020, 4, 23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = yf.Ticker('MSFT')\n",
    "hist = tic.history(start=START, end=END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Dividends</th>\n",
       "      <th>Stock Splits</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1986-03-13</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>1031788800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1986-03-14</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>308160000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1986-03-17</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.07</td>\n",
       "      <td>133171200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1986-03-18</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>67766400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1986-03-19</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>47894400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-04-16</td>\n",
       "      <td>173.82</td>\n",
       "      <td>176.79</td>\n",
       "      <td>172.42</td>\n",
       "      <td>176.55</td>\n",
       "      <td>50479600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-04-17</td>\n",
       "      <td>179.00</td>\n",
       "      <td>179.50</td>\n",
       "      <td>175.38</td>\n",
       "      <td>178.10</td>\n",
       "      <td>52765600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-04-20</td>\n",
       "      <td>176.14</td>\n",
       "      <td>178.25</td>\n",
       "      <td>174.50</td>\n",
       "      <td>174.57</td>\n",
       "      <td>36669600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-04-21</td>\n",
       "      <td>173.02</td>\n",
       "      <td>173.19</td>\n",
       "      <td>165.65</td>\n",
       "      <td>167.35</td>\n",
       "      <td>56203700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-04-22</td>\n",
       "      <td>170.91</td>\n",
       "      <td>173.52</td>\n",
       "      <td>170.35</td>\n",
       "      <td>173.04</td>\n",
       "      <td>34651600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8598 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Open    High     Low   Close      Volume  Dividends  \\\n",
       "Date                                                                \n",
       "1986-03-13    0.06    0.06    0.06    0.06  1031788800        0.0   \n",
       "1986-03-14    0.06    0.07    0.06    0.06   308160000        0.0   \n",
       "1986-03-17    0.06    0.07    0.06    0.07   133171200        0.0   \n",
       "1986-03-18    0.07    0.07    0.06    0.06    67766400        0.0   \n",
       "1986-03-19    0.06    0.06    0.06    0.06    47894400        0.0   \n",
       "...            ...     ...     ...     ...         ...        ...   \n",
       "2020-04-16  173.82  176.79  172.42  176.55    50479600        0.0   \n",
       "2020-04-17  179.00  179.50  175.38  178.10    52765600        0.0   \n",
       "2020-04-20  176.14  178.25  174.50  174.57    36669600        0.0   \n",
       "2020-04-21  173.02  173.19  165.65  167.35    56203700        0.0   \n",
       "2020-04-22  170.91  173.52  170.35  173.04    34651600        0.0   \n",
       "\n",
       "            Stock Splits  \n",
       "Date                      \n",
       "1986-03-13           0.0  \n",
       "1986-03-14           0.0  \n",
       "1986-03-17           0.0  \n",
       "1986-03-18           0.0  \n",
       "1986-03-19           0.0  \n",
       "...                  ...  \n",
       "2020-04-16           0.0  \n",
       "2020-04-17           0.0  \n",
       "2020-04-20           0.0  \n",
       "2020-04-21           0.0  \n",
       "2020-04-22           0.0  \n",
       "\n",
       "[8598 rows x 7 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dta_transformation(data, est_h):\n",
    "    # Make sure data has sufficient columns\n",
    "    assert 'Open' in data.columns\n",
    "    assert 'High' in data.columns\n",
    "    assert 'Low' in data.columns\n",
    "    assert 'Close' in data.columns\n",
    "\n",
    "    data['lag_close'] = data['Close'].shift(1)\n",
    "    data['Indicator'] = np.where(data['Close'] > data['lag_close'], 1, 0)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in range(est_h, data.shape[0]):\n",
    "        sub_dta = data.iloc[i - est_h:i]\n",
    "\n",
    "        y_i = data.iloc[i]['Indicator']\n",
    "        x_i = dta_to_candlestick(sub_dta)\n",
    "\n",
    "        y.append(y_i)\n",
    "        x.append(x_i)\n",
    "\n",
    "        print(\"{}/{}\".format(i - est_h, data.shape[0] - est_h))\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Dividends</th>\n",
       "      <th>Stock Splits</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2006-02-03</td>\n",
       "      <td>20.07</td>\n",
       "      <td>20.23</td>\n",
       "      <td>19.97</td>\n",
       "      <td>20.11</td>\n",
       "      <td>75022700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2006-02-06</td>\n",
       "      <td>20.09</td>\n",
       "      <td>20.11</td>\n",
       "      <td>19.79</td>\n",
       "      <td>19.84</td>\n",
       "      <td>60170500</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2006-02-07</td>\n",
       "      <td>19.68</td>\n",
       "      <td>19.83</td>\n",
       "      <td>19.58</td>\n",
       "      <td>19.68</td>\n",
       "      <td>72159500</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2006-02-08</td>\n",
       "      <td>19.73</td>\n",
       "      <td>19.78</td>\n",
       "      <td>19.51</td>\n",
       "      <td>19.65</td>\n",
       "      <td>51795200</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2006-02-09</td>\n",
       "      <td>19.69</td>\n",
       "      <td>19.74</td>\n",
       "      <td>19.46</td>\n",
       "      <td>19.47</td>\n",
       "      <td>52861700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2006-02-10</td>\n",
       "      <td>19.44</td>\n",
       "      <td>19.64</td>\n",
       "      <td>19.36</td>\n",
       "      <td>19.49</td>\n",
       "      <td>52127000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2006-02-13</td>\n",
       "      <td>19.45</td>\n",
       "      <td>19.50</td>\n",
       "      <td>19.24</td>\n",
       "      <td>19.27</td>\n",
       "      <td>46707000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2006-02-14</td>\n",
       "      <td>19.29</td>\n",
       "      <td>19.49</td>\n",
       "      <td>19.24</td>\n",
       "      <td>19.46</td>\n",
       "      <td>58432900</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2006-02-15</td>\n",
       "      <td>19.49</td>\n",
       "      <td>19.73</td>\n",
       "      <td>19.42</td>\n",
       "      <td>19.70</td>\n",
       "      <td>62808900</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2006-02-16</td>\n",
       "      <td>19.68</td>\n",
       "      <td>19.71</td>\n",
       "      <td>19.47</td>\n",
       "      <td>19.65</td>\n",
       "      <td>48868500</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2006-02-17</td>\n",
       "      <td>19.54</td>\n",
       "      <td>19.65</td>\n",
       "      <td>19.46</td>\n",
       "      <td>19.57</td>\n",
       "      <td>41513200</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2006-02-21</td>\n",
       "      <td>19.58</td>\n",
       "      <td>19.58</td>\n",
       "      <td>19.30</td>\n",
       "      <td>19.45</td>\n",
       "      <td>50216100</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2006-02-22</td>\n",
       "      <td>19.44</td>\n",
       "      <td>19.68</td>\n",
       "      <td>19.40</td>\n",
       "      <td>19.58</td>\n",
       "      <td>43043100</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2006-02-23</td>\n",
       "      <td>19.59</td>\n",
       "      <td>19.71</td>\n",
       "      <td>19.45</td>\n",
       "      <td>19.54</td>\n",
       "      <td>47359100</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2006-02-24</td>\n",
       "      <td>19.49</td>\n",
       "      <td>19.60</td>\n",
       "      <td>19.43</td>\n",
       "      <td>19.52</td>\n",
       "      <td>44753800</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2006-02-27</td>\n",
       "      <td>19.60</td>\n",
       "      <td>19.98</td>\n",
       "      <td>19.54</td>\n",
       "      <td>19.82</td>\n",
       "      <td>51301500</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2006-02-28</td>\n",
       "      <td>19.75</td>\n",
       "      <td>20.01</td>\n",
       "      <td>19.69</td>\n",
       "      <td>19.69</td>\n",
       "      <td>65036100</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2006-03-01</td>\n",
       "      <td>19.77</td>\n",
       "      <td>19.93</td>\n",
       "      <td>19.75</td>\n",
       "      <td>19.89</td>\n",
       "      <td>53061200</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2006-03-02</td>\n",
       "      <td>19.80</td>\n",
       "      <td>19.86</td>\n",
       "      <td>19.71</td>\n",
       "      <td>19.76</td>\n",
       "      <td>41850300</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2006-03-03</td>\n",
       "      <td>19.65</td>\n",
       "      <td>19.90</td>\n",
       "      <td>19.60</td>\n",
       "      <td>19.73</td>\n",
       "      <td>45218800</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2006-03-06</td>\n",
       "      <td>19.73</td>\n",
       "      <td>19.90</td>\n",
       "      <td>19.66</td>\n",
       "      <td>19.72</td>\n",
       "      <td>53054100</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2006-03-07</td>\n",
       "      <td>19.71</td>\n",
       "      <td>19.86</td>\n",
       "      <td>19.65</td>\n",
       "      <td>19.83</td>\n",
       "      <td>51613900</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2006-03-08</td>\n",
       "      <td>19.78</td>\n",
       "      <td>20.15</td>\n",
       "      <td>19.76</td>\n",
       "      <td>19.97</td>\n",
       "      <td>57547400</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2006-03-09</td>\n",
       "      <td>19.98</td>\n",
       "      <td>20.09</td>\n",
       "      <td>19.79</td>\n",
       "      <td>19.79</td>\n",
       "      <td>45360700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2006-03-10</td>\n",
       "      <td>19.83</td>\n",
       "      <td>19.95</td>\n",
       "      <td>19.70</td>\n",
       "      <td>19.91</td>\n",
       "      <td>41297200</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2006-03-13</td>\n",
       "      <td>19.92</td>\n",
       "      <td>20.00</td>\n",
       "      <td>19.74</td>\n",
       "      <td>19.87</td>\n",
       "      <td>40342600</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2006-03-14</td>\n",
       "      <td>19.82</td>\n",
       "      <td>20.06</td>\n",
       "      <td>19.78</td>\n",
       "      <td>19.95</td>\n",
       "      <td>39821800</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2006-03-15</td>\n",
       "      <td>19.93</td>\n",
       "      <td>20.12</td>\n",
       "      <td>19.79</td>\n",
       "      <td>20.05</td>\n",
       "      <td>57152000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2006-03-16</td>\n",
       "      <td>20.04</td>\n",
       "      <td>20.14</td>\n",
       "      <td>19.95</td>\n",
       "      <td>19.98</td>\n",
       "      <td>73793700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2006-03-17</td>\n",
       "      <td>20.04</td>\n",
       "      <td>20.27</td>\n",
       "      <td>19.98</td>\n",
       "      <td>20.15</td>\n",
       "      <td>120615000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Open   High    Low  Close     Volume  Dividends  Stock Splits\n",
       "Date                                                                      \n",
       "2006-02-03  20.07  20.23  19.97  20.11   75022700       0.00           0.0\n",
       "2006-02-06  20.09  20.11  19.79  19.84   60170500       0.00           0.0\n",
       "2006-02-07  19.68  19.83  19.58  19.68   72159500       0.00           0.0\n",
       "2006-02-08  19.73  19.78  19.51  19.65   51795200       0.00           0.0\n",
       "2006-02-09  19.69  19.74  19.46  19.47   52861700       0.00           0.0\n",
       "2006-02-10  19.44  19.64  19.36  19.49   52127000       0.00           0.0\n",
       "2006-02-13  19.45  19.50  19.24  19.27   46707000       0.00           0.0\n",
       "2006-02-14  19.29  19.49  19.24  19.46   58432900       0.00           0.0\n",
       "2006-02-15  19.49  19.73  19.42  19.70   62808900       0.09           0.0\n",
       "2006-02-16  19.68  19.71  19.47  19.65   48868500       0.00           0.0\n",
       "2006-02-17  19.54  19.65  19.46  19.57   41513200       0.00           0.0\n",
       "2006-02-21  19.58  19.58  19.30  19.45   50216100       0.00           0.0\n",
       "2006-02-22  19.44  19.68  19.40  19.58   43043100       0.00           0.0\n",
       "2006-02-23  19.59  19.71  19.45  19.54   47359100       0.00           0.0\n",
       "2006-02-24  19.49  19.60  19.43  19.52   44753800       0.00           0.0\n",
       "2006-02-27  19.60  19.98  19.54  19.82   51301500       0.00           0.0\n",
       "2006-02-28  19.75  20.01  19.69  19.69   65036100       0.00           0.0\n",
       "2006-03-01  19.77  19.93  19.75  19.89   53061200       0.00           0.0\n",
       "2006-03-02  19.80  19.86  19.71  19.76   41850300       0.00           0.0\n",
       "2006-03-03  19.65  19.90  19.60  19.73   45218800       0.00           0.0\n",
       "2006-03-06  19.73  19.90  19.66  19.72   53054100       0.00           0.0\n",
       "2006-03-07  19.71  19.86  19.65  19.83   51613900       0.00           0.0\n",
       "2006-03-08  19.78  20.15  19.76  19.97   57547400       0.00           0.0\n",
       "2006-03-09  19.98  20.09  19.79  19.79   45360700       0.00           0.0\n",
       "2006-03-10  19.83  19.95  19.70  19.91   41297200       0.00           0.0\n",
       "2006-03-13  19.92  20.00  19.74  19.87   40342600       0.00           0.0\n",
       "2006-03-14  19.82  20.06  19.78  19.95   39821800       0.00           0.0\n",
       "2006-03-15  19.93  20.12  19.79  20.05   57152000       0.00           0.0\n",
       "2006-03-16  20.04  20.14  19.95  19.98   73793700       0.00           0.0\n",
       "2006-03-17  20.04  20.27  19.98  20.15  120615000       0.00           0.0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = hist.iloc[5020:5050]\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.where(test_data['Close'] > test_data['Open'], 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "count_1 = 0\n",
    "count_0 = 0\n",
    "\n",
    "for i in range(len(temp)-1):\n",
    "    if (temp[i] == 1) & (temp[i+1] == 1):\n",
    "        count_1 += 1\n",
    "\n",
    "    elif (temp[i] == 0) & (temp[i+1] == 0):\n",
    "        print(i)\n",
    "        count_0 += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,\n",
       "        1, 0, 1, 0, 1, 1, 0, 1]), 4, 3)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp, count_1, count_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test_data['Close'] <= 19.8).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid(data):\n",
    "    # check if the entry is valid\n",
    "    if isinstance(data, float):\n",
    "        return data\n",
    "    else:\n",
    "        return 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def momentum_ret(data):\n",
    "    x_0 = is_valid(data['Close'][0])\n",
    "    x_T = is_valid(data['Close'][-1])\n",
    "    mid = data.shape[0] // 2\n",
    "    x_t = is_valid(data['Close'][mid])\n",
    "    \n",
    "    ttl_ret = (x_T - x_0) / x_0\n",
    "    half_ret = (x_t - x_0) / x_0\n",
    "    return ttl_ret + half_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_cutoff(data):\n",
    "    mu = data['Close'].mean()\n",
    "    upr = (data['Close'] > mu).sum()\n",
    "    lwr = (data['Close'] <= mu).sum()\n",
    "    return upr - lwr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def half_return_diff(data):\n",
    "    x_0 = is_valid(data['Close'][0])\n",
    "    x_T = is_valid(data['Close'][-1])\n",
    "    mid = data.shape[0] // 2\n",
    "    x_t = is_valid(data['Close'][mid])\n",
    "    \n",
    "    first_ret = (x_t - x_0) / x_0\n",
    "    second_ret = (x_T - x_t) / x_t\n",
    "    return second_ret - first_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consec_trend(data):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.12915851272015658"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test_data['Close'][-1] - test_data['Close'][0]) / test_data['Close'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid_point = test_data.shape[0] // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.10999999999999996"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test_data['Close'][-1] - test_data['Close'][mid_point]) / test_data['Close'][mid_point]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andyy\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\Andyy\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/20\n",
      "1/20\n",
      "2/20\n",
      "3/20\n",
      "4/20\n",
      "5/20\n",
      "6/20\n",
      "7/20\n",
      "8/20\n",
      "9/20\n",
      "10/20\n",
      "11/20\n",
      "12/20\n",
      "13/20\n",
      "14/20\n",
      "15/20\n",
      "16/20\n",
      "17/20\n",
      "18/20\n",
      "19/20\n"
     ]
    }
   ],
   "source": [
    "x, y = dta_transformation(hist.iloc[5000:5050], 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.stack(x, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 512, 990)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = Image.fromarray(x[:,:,0])\n",
    "im.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_x = x[:,:,568:1568]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(990,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array(y)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_y = y[568:1568]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('test_1', x=sub_x, y=sub_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "568"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8568 % 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape[2] // 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    sub_x = x[:,:,(1000*i+568):(1000*(i+1)+568)]\n",
    "    sub_y = y[(1000*i+568):(1000*(i+1)+568)]\n",
    "    np.savez_compressed('test_{}'.format(i), x=sub_x, y=sub_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "333.7860107421875"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "350000000/1024/1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ResNet_CNN import *\n",
    "\n",
    "import math as m\n",
    "import torch\n",
    "from torch.nn import Linear, ReLU, Conv1d, Conv2d, Flatten, Sequential, CrossEntropyLoss, MSELoss, MaxPool1d, MaxPool2d, Dropout, BatchNorm1d, BatchNorm2d\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from functools import partial\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = res_conv1(1, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (encoder): ResNetEncoder(\n",
      "    (gate): Sequential(\n",
      "      (0): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (blocks): ModuleList(\n",
      "      (0): ResNetLayer(\n",
      "        (blocks): Sequential(\n",
      "          (0): ResNetBasicBlock(\n",
      "            (blocks): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Conv2dAuto(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "              (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "              (2): Sequential(\n",
      "                (0): Conv2dAuto(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (activate): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "            (shortcut): None\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): ResNetLayer(\n",
      "        (blocks): Sequential(\n",
      "          (0): ResNetBasicBlock(\n",
      "            (blocks): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Conv2dAuto(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "              (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "              (2): Sequential(\n",
      "                (0): Conv2dAuto(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (activate): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "            (shortcut): Sequential(\n",
      "              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): ResNetDecoder(\n",
      "    (avg): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (decoder): ModuleList(\n",
      "      (0): Linear(in_features=128, out_features=100, bias=True)\n",
      "      (1): Dropout2d(p=0.5, inplace=False)\n",
      "      (2): ReLU()\n",
      "      (3): Linear(in_features=100, out_features=25, bias=True)\n",
      "      (4): Dropout2d(p=0.5, inplace=False)\n",
      "      (5): ReLU()\n",
      "      (6): Linear(in_features=25, out_features=1, bias=True)\n",
      "      (7): Sigmoid()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "â”œâ”€ResNetEncoder: 1-1                     [-1, 128, 64, 64]         --\n",
      "|    â””â”€Sequential: 2-1                   [-1, 64, 128, 128]        --\n",
      "|    |    â””â”€Conv2d: 3-1                  [-1, 64, 256, 256]        3,136\n",
      "|    |    â””â”€BatchNorm2d: 3-2             [-1, 64, 256, 256]        128\n",
      "|    |    â””â”€LeakyReLU: 3-3               [-1, 64, 256, 256]        --\n",
      "|    |    â””â”€MaxPool2d: 3-4               [-1, 64, 128, 128]        --\n",
      "â”œâ”€ResNetDecoder: 1-2                     [-1, 1]                   --\n",
      "|    â””â”€AdaptiveAvgPool2d: 2-2            [-1, 128, 1, 1]           --\n",
      "==========================================================================================\n",
      "Total params: 322,843\n",
      "Trainable params: 322,843\n",
      "Non-trainable params: 0\n",
      "------------------------------------------------------------------------------------------\n",
      "Input size (MB): 1.00\n",
      "Forward/backward pass size (MB): 64.00\n",
      "Params size (MB): 1.23\n",
      "Estimated Total Size (MB): 66.23\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------------------------------------------------------------\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "â”œâ”€ResNetEncoder: 1-1                     [-1, 128, 64, 64]         --\n",
      "|    â””â”€Sequential: 2-1                   [-1, 64, 128, 128]        --\n",
      "|    |    â””â”€Conv2d: 3-1                  [-1, 64, 256, 256]        3,136\n",
      "|    |    â””â”€BatchNorm2d: 3-2             [-1, 64, 256, 256]        128\n",
      "|    |    â””â”€LeakyReLU: 3-3               [-1, 64, 256, 256]        --\n",
      "|    |    â””â”€MaxPool2d: 3-4               [-1, 64, 128, 128]        --\n",
      "â”œâ”€ResNetDecoder: 1-2                     [-1, 1]                   --\n",
      "|    â””â”€AdaptiveAvgPool2d: 2-2            [-1, 128, 1, 1]           --\n",
      "==========================================================================================\n",
      "Total params: 322,843\n",
      "Trainable params: 322,843\n",
      "Non-trainable params: 0\n",
      "------------------------------------------------------------------------------------------\n",
      "Input size (MB): 1.00\n",
      "Forward/backward pass size (MB): 64.00\n",
      "Params size (MB): 1.23\n",
      "Estimated Total Size (MB): 66.23\n",
      "------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(summary(model, (1,512,512)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_list = os.listdir('D:/GitHub/Backtesting/images_npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\GitHub\\\\Backtesting'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_dta = os.listdir('D:/GitHub/Backtesting/images_npy/{}'.format(ticker_list[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CI', 'DDS', 'IPG', 'LLY', 'MTG', 'NAVI', 'NKE', 'SEQ', 'WY']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticker_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CI_0.npz',\n",
       " 'CI_1.npz',\n",
       " 'CI_2.npz',\n",
       " 'CI_3.npz',\n",
       " 'CI_4.npz',\n",
       " 'CI_5.npz',\n",
       " 'CI_6.npz',\n",
       " 'CI_7.npz',\n",
       " 'CI_8.npz']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticker_dta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.load('C:/Users/Andyy/Documents/GitHub/Backtesting/images_npy/{}/{}'.format(ticker_list[0], ticker_dta[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dta_x = temp['x']\n",
    "dta_y = temp['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = res_conv1(1, 64)\n",
    "lr = 0.0001\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = np.load('test.npz')\n",
    "dta_x = loaded['x']\n",
    "dta_y = loaded['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((891, 512, 512), (891, 1)) ((99, 512, 512), (99, 1))\n"
     ]
    }
   ],
   "source": [
    "train_X, train_Y, val_X, val_Y = data_preprocessing(dta_x, dta_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(990,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dta_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('test', x=x, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataLoader(path):\n",
    "    loaded = np.load(path)\n",
    "    dta_x = loaded['x']\n",
    "    dta_y = loaded['y']\n",
    "    return dta_x, dta_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on CI_0.npz\n",
      "((900, 256, 256), (900, 1)) ((100, 256, 256), (100, 1))\n",
      "Epoch:  1 \t train loss:  tensor(0.7072, grad_fn=<BinaryCrossEntropyBackward>) \t val loss:  tensor(0.7103, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train on CI_1.npz\n",
      "((900, 256, 256), (900, 1)) ((100, 256, 256), (100, 1))\n",
      "Epoch:  1 \t train loss:  tensor(0.7052, grad_fn=<BinaryCrossEntropyBackward>) \t val loss:  tensor(0.7158, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train on CI_2.npz\n",
      "((900, 256, 256), (900, 1)) ((100, 256, 256), (100, 1))\n",
      "Epoch:  1 \t train loss:  tensor(0.7063, grad_fn=<BinaryCrossEntropyBackward>) \t val loss:  tensor(0.7024, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train on CI_3.npz\n",
      "((900, 256, 256), (900, 1)) ((100, 256, 256), (100, 1))\n",
      "Epoch:  1 \t train loss:  tensor(0.6973, grad_fn=<BinaryCrossEntropyBackward>) \t val loss:  tensor(0.7002, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train on CI_4.npz\n",
      "((900, 256, 256), (900, 1)) ((100, 256, 256), (100, 1))\n",
      "Epoch:  1 \t train loss:  tensor(0.6984, grad_fn=<BinaryCrossEntropyBackward>) \t val loss:  tensor(0.6930, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train on CI_5.npz\n",
      "((900, 256, 256), (900, 1)) ((100, 256, 256), (100, 1))\n",
      "Epoch:  1 \t train loss:  tensor(0.6921, grad_fn=<BinaryCrossEntropyBackward>) \t val loss:  tensor(0.6879, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train on CI_6.npz\n",
      "((900, 256, 256), (900, 1)) ((100, 256, 256), (100, 1))\n",
      "Epoch:  1 \t train loss:  tensor(0.6989, grad_fn=<BinaryCrossEntropyBackward>) \t val loss:  tensor(0.7002, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train on CI_7.npz\n",
      "((900, 256, 256), (900, 1)) ((100, 256, 256), (100, 1))\n",
      "Epoch:  1 \t train loss:  tensor(0.6953, grad_fn=<BinaryCrossEntropyBackward>) \t val loss:  tensor(0.6865, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train on CI_8.npz\n",
      "((900, 256, 256), (900, 1)) ((100, 256, 256), (100, 1))\n",
      "Epoch:  1 \t train loss:  tensor(0.6957, grad_fn=<BinaryCrossEntropyBackward>) \t val loss:  tensor(0.6966, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train on DDS_0.npz\n",
      "((900, 256, 256), (900, 1)) ((100, 256, 256), (100, 1))\n",
      "Epoch:  1 \t train loss:  tensor(0.7137, grad_fn=<BinaryCrossEntropyBackward>) \t val loss:  tensor(0.7180, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train on DDS_1.npz\n",
      "((900, 256, 256), (900, 1)) ((100, 256, 256), (100, 1))\n",
      "Epoch:  1 \t train loss:  tensor(0.7021, grad_fn=<BinaryCrossEntropyBackward>) \t val loss:  tensor(0.7007, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train on DDS_2.npz\n",
      "((900, 256, 256), (900, 1)) ((100, 256, 256), (100, 1))\n",
      "Epoch:  1 \t train loss:  tensor(0.6975, grad_fn=<BinaryCrossEntropyBackward>) \t val loss:  tensor(0.7088, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train on DDS_3.npz\n",
      "((900, 256, 256), (900, 1)) ((100, 256, 256), (100, 1))\n",
      "Epoch:  1 \t train loss:  tensor(0.7009, grad_fn=<BinaryCrossEntropyBackward>) \t val loss:  tensor(0.7019, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train on DDS_4.npz\n",
      "((900, 256, 256), (900, 1)) ((100, 256, 256), (100, 1))\n",
      "Epoch:  1 \t train loss:  tensor(0.6996, grad_fn=<BinaryCrossEntropyBackward>) \t val loss:  tensor(0.7060, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train on DDS_5.npz\n",
      "((900, 256, 256), (900, 1)) ((100, 256, 256), (100, 1))\n",
      "Epoch:  1 \t train loss:  tensor(0.6959, grad_fn=<BinaryCrossEntropyBackward>) \t val loss:  tensor(0.7065, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train on DDS_6.npz\n",
      "((900, 256, 256), (900, 1)) ((100, 256, 256), (100, 1))\n",
      "Epoch:  1 \t train loss:  tensor(0.6959, grad_fn=<BinaryCrossEntropyBackward>) \t val loss:  tensor(0.6940, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train on DDS_7.npz\n",
      "((900, 256, 256), (900, 1)) ((100, 256, 256), (100, 1))\n",
      "Epoch:  1 \t train loss:  tensor(0.6905, grad_fn=<BinaryCrossEntropyBackward>) \t val loss:  tensor(0.6866, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train on DDS_8.npz\n",
      "((900, 256, 256), (900, 1)) ((100, 256, 256), (100, 1))\n",
      "Epoch:  1 \t train loss:  tensor(0.6950, grad_fn=<BinaryCrossEntropyBackward>) \t val loss:  tensor(0.6877, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train on DDS_9.npz\n",
      "((900, 256, 256), (900, 1)) ((100, 256, 256), (100, 1))\n",
      "Epoch:  1 \t train loss:  tensor(0.6940, grad_fn=<BinaryCrossEntropyBackward>) \t val loss:  tensor(0.6959, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train on IPG_0.npz\n",
      "((900, 256, 256), (900, 1)) ((100, 256, 256), (100, 1))\n",
      "Epoch:  1 \t train loss:  tensor(0.7279, grad_fn=<BinaryCrossEntropyBackward>) \t val loss:  tensor(0.7291, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train on IPG_1.npz\n",
      "((900, 256, 256), (900, 1)) ((100, 256, 256), (100, 1))\n",
      "Epoch:  1 \t train loss:  tensor(0.7107, grad_fn=<BinaryCrossEntropyBackward>) \t val loss:  tensor(0.7014, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train on IPG_2.npz\n",
      "((900, 256, 256), (900, 1)) ((100, 256, 256), (100, 1))\n",
      "Epoch:  1 \t train loss:  tensor(0.7010, grad_fn=<BinaryCrossEntropyBackward>) \t val loss:  tensor(0.6975, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train on IPG_3.npz\n",
      "((900, 256, 256), (900, 1)) ((100, 256, 256), (100, 1))\n",
      "Epoch:  1 \t train loss:  tensor(0.6998, grad_fn=<BinaryCrossEntropyBackward>) \t val loss:  tensor(0.6918, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train on IPG_4.npz\n",
      "((900, 256, 256), (900, 1)) ((100, 256, 256), (100, 1))\n",
      "Epoch:  1 \t train loss:  tensor(0.6949, grad_fn=<BinaryCrossEntropyBackward>) \t val loss:  tensor(0.6933, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train on IPG_5.npz\n",
      "((900, 256, 256), (900, 1)) ((100, 256, 256), (100, 1))\n",
      "Epoch:  1 \t train loss:  tensor(0.6945, grad_fn=<BinaryCrossEntropyBackward>) \t val loss:  tensor(0.6873, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train on IPG_6.npz\n",
      "((900, 256, 256), (900, 1)) ((100, 256, 256), (100, 1))\n",
      "Epoch:  1 \t train loss:  tensor(0.6952, grad_fn=<BinaryCrossEntropyBackward>) \t val loss:  tensor(0.6954, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train on IPG_7.npz\n",
      "((900, 256, 256), (900, 1)) ((100, 256, 256), (100, 1))\n",
      "Epoch:  1 \t train loss:  tensor(0.6965, grad_fn=<BinaryCrossEntropyBackward>) \t val loss:  tensor(0.6922, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train on IPG_8.npz\n",
      "((900, 256, 256), (900, 1)) ((100, 256, 256), (100, 1))\n",
      "Epoch:  1 \t train loss:  tensor(0.6944, grad_fn=<BinaryCrossEntropyBackward>) \t val loss:  tensor(0.6971, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train on IPG_9.npz\n",
      "((900, 256, 256), (900, 1)) ((100, 256, 256), (100, 1))\n",
      "Epoch:  1 \t train loss:  tensor(0.6929, grad_fn=<BinaryCrossEntropyBackward>) \t val loss:  tensor(0.6889, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train on LLY_0.npz\n",
      "((900, 256, 256), (900, 1)) ((100, 256, 256), (100, 1))\n",
      "Epoch:  1 \t train loss:  tensor(0.7062, grad_fn=<BinaryCrossEntropyBackward>) \t val loss:  tensor(0.7019, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train on LLY_1.npz\n",
      "((900, 256, 256), (900, 1)) ((100, 256, 256), (100, 1))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at ..\\c10\\core\\CPUAllocator.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 471859200 bytes. Buy new RAM!\n(no backtrace available)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-f482e1fd9ff4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-f1e3f7e9326a>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(epochs)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mval_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mloss_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m         \"\"\"\n\u001b[1;32m--> 198\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at ..\\c10\\core\\CPUAllocator.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 471859200 bytes. Buy new RAM!\n(no backtrace available)"
     ]
    }
   ],
   "source": [
    "for comp in ticker_list:\n",
    "    ticker_dta = os.listdir('D:/GitHub/Backtesting/images_npy/{}'.format(comp))\n",
    "    for dta in ticker_dta:\n",
    "        try:\n",
    "            path = 'D:/GitHub/Backtesting/images_npy/{}/{}'.format(comp, dta)\n",
    "            dta_x, dta_y = dataLoader(path)\n",
    "            print(\"Train on {}\".format(dta))\n",
    "            # Begin training\n",
    "            \n",
    "            train_X, train_Y, val_X, val_Y = data_preprocessing(dta_x, dta_y)\n",
    "            gc.collect()\n",
    "            \n",
    "            train(0)\n",
    "        except RuntimeError:\n",
    "            model_path = './cnn_res.pth'\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            print(\"{} breaks the computer!!!\".format(dta))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './cnn_res.pth'\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = res_conv1(1,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(X, Y):\n",
    "    X = X.reshape((-1, 512, 512)).astype(np.float32)\n",
    "    Y = Y.reshape((-1, 1)).astype(np.float32)\n",
    "    train_X, val_X, train_Y, val_Y = train_test_split(X, Y, test_size=0.1, random_state=42)\n",
    "    print((train_X.shape, train_Y.shape), (val_X.shape, val_Y.shape))\n",
    "\n",
    "    train_X = train_X.reshape(-1, 1, 512, 512)\n",
    "    train_X = torch.from_numpy(train_X)\n",
    "    train_Y = torch.from_numpy(train_Y)\n",
    "\n",
    "    val_X =  val_X.reshape(-1, 1, 512, 512)\n",
    "    val_X = torch.from_numpy(val_X)\n",
    "    val_Y = torch.from_numpy(val_Y)\n",
    "    \n",
    "    return train_X, train_Y, val_X, val_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = transforms.Compose([transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((900, 256, 256), (900, 1))((100, 256, 256), (100, 1))\n"
     ]
    }
   ],
   "source": [
    "train_X, train_Y, val_X, val_Y = data_preprocessing(dta_x, dta_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = res_conv1(1, 64)\n",
    "lr = 0.0001\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs):\n",
    "    # dataset\n",
    "    x_train, y_train = Variable(train_X), Variable(train_Y)\n",
    "    x_val, y_val = Variable(val_X), Variable(val_Y)\n",
    " \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    output_train = model(x_train)\n",
    "    output_val = model(x_val)\n",
    "    \n",
    "    loss_train = criterion(output_train, y_train.type(torch.float))\n",
    "    loss_val = criterion(output_val, y_val.type(torch.float))\n",
    "    \n",
    "    train_losses.append(loss_train)\n",
    "    val_losses.append(loss_val)\n",
    "    \n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print('Epoch: ', epochs+1, '\\t', 'train loss: ', loss_train, '\\t', 'val loss: ', loss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breaks the computer!!!\n"
     ]
    }
   ],
   "source": [
    "model_path = './cnn_res-1.pth'\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(\"Breaks the computer!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breaks the computer!!!\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 2\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "tr_loss = 0\n",
    "try:\n",
    "    for epochs in range(n_epochs):\n",
    "        train(epochs)\n",
    "except RuntimeError:\n",
    "    model_path = './cnn_res-1.pth'\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(\"Breaks the computer!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
