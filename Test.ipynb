{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yfinance as yf\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import pytz\n",
    "import logging\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "START = datetime(1980, 1, 1)\n",
    "END = datetime(2020, 4, 23)\n",
    "msft = yf.Ticker(\"MSFT\")\n",
    "data = msft.history(start=START, end=END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['lag_close'] = data['Close'].shift(1)\n",
    "data['Indicator'] = np.where(data['Close'] > data['lag_close'], 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_dta = data.iloc[-30:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dta_to_candlestick(data):\n",
    "    l = len(data)\n",
    "    # Make candlestick picture\n",
    "    layout = go.Layout(xaxis=dict(ticks='',\n",
    "                                  showgrid=False,\n",
    "                                  showticklabels=False,\n",
    "                                  rangeslider=dict(visible=False)),\n",
    "                       yaxis=dict(ticks='',\n",
    "                                  showgrid=False,\n",
    "                                  showticklabels=False),\n",
    "                       #width=500,\n",
    "                       #height=500,\n",
    "                       paper_bgcolor='rgba(0,0,0,0)',\n",
    "                       plot_bgcolor='rgba(0,0,0,0)')\n",
    "    fig = go.Figure(data=[go.Candlestick(x=np.linspace(1,l,l),\n",
    "                                         open=data.Open,\n",
    "                                         high=data.High,\n",
    "                                         low=data.Low,\n",
    "                                         close=data.Close)],\n",
    "                    layout=layout)\n",
    "    fig.write_image(\"images/fig.png\")\n",
    "\n",
    "    # Convert to numpy array\n",
    "    im = Image.open('images/fig.png')\n",
    "    data = np.asarray(im)\n",
    "\n",
    "    # Return the first channel of the image\n",
    "    return data[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_dta = dta_to_candlestick(sub_dta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "for i in range(30, data.shape[0]):\n",
    "    sub_dta = data.iloc[i - 30:i]\n",
    "\n",
    "    y_i = data.iloc[i]['Indicator']\n",
    "    x_i = dta_to_candlestick(sub_dta)\n",
    "\n",
    "    y.append(y_i)\n",
    "    x.append(x_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dta_x = np.stack(x, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 700, 8568)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dta_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf = h5py.File('data.h5', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"dataset_1\": shape (500, 700, 8568), type \"|u1\">"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf.create_dataset('dataset_1', data=dta_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('MSFT_x.npy', dta_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('MSFT_test', a=dta_x, b=np.array(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = np.load('MSFT_test.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dta_x = loaded['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 700, 8568)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dta_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8568,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dta_y = loaded['b']\n",
    "dta_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Machine Learning Shit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math as m\n",
    "import torch\n",
    "from torch.nn import Linear, ReLU, Conv1d, Conv2d, Flatten, Sequential, CrossEntropyLoss, MSELoss, MaxPool1d, MaxPool2d, Dropout, BatchNorm1d, BatchNorm2d\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch import nn\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2dAuto(nn.Conv2d):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.padding = (self.kernel_size[0] // 2, self.kernel_size[1] // 2)\n",
    "        \n",
    "conv3x3 = partial(Conv2dAuto, kernel_size=3, bias=False)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Conv2dAuto(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"
    }
   ],
   "source": [
    "conv = conv3x3(in_channels=32, out_channels=64)\n",
    "print(conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_func(activation):\n",
    "    return nn.ModuleDict([\n",
    "        ['relu', nn.ReLU(inplace=True)],\n",
    "        ['leaky_relu', nn.LeakyReLU(negative_slope=0.01, inplace=True)],\n",
    "        ['selu', nn.SELU(inplace=True)],\n",
    "        ['none', nn.Identity()]\n",
    "    ])[activation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, activation='relu'):\n",
    "        super().__init__()\n",
    "        self.in_channels, self.out_channels, self.activation = in_channels, out_channels, activation\n",
    "        self.blocks = nn.Identity()\n",
    "        self.activate = activation_func(activation)\n",
    "        self.shortcut = nn.Identity()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        if self.should_apply_shortcut: residual = self.shortcut(x)\n",
    "        x = self.blocks(x)\n",
    "        x += residual\n",
    "        x = self.activate(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    @property\n",
    "    def should_apply_shortcut(self):\n",
    "        return self.in_channels != self.out_channels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetResidualBlock(ResidualBlock):\n",
    "    def __init__(self, in_channels, out_channels, expansion=1, downsampling=1, conv=conv3x3, *args, **kwargs):\n",
    "        super().__init__(in_channels, out_channels, *args, **kwargs)\n",
    "        self.expansion, self.downsampling, self.conv = expansion, downsampling, conv\n",
    "        self.shortcut = nn.Sequential(\n",
    "            nn.Conv2d(self.in_channels, self.expanded_channels, kernel_size=1, stride=self.downsampling, bias=False),\n",
    "            nn.BatchNorm2d(self.expanded_channels)) if self.should_apply_shortcut else None\n",
    "    \n",
    "    @property\n",
    "    def expanded_channels(self):\n",
    "        return self.out_channels * self.expansion\n",
    "    \n",
    "    @property\n",
    "    def should_apply_shortcut(self):\n",
    "        return self.in_channels != self.expanded_channels\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "ResNetResidualBlock(\n  (blocks): Identity()\n  (activate): ReLU(inplace=True)\n  (shortcut): Sequential(\n    (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n)"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "ResNetResidualBlock(32, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_bn(in_channels, out_channels, conv, *args, **kwargs):\n",
    "    return nn.Sequential(conv(in_channels, out_channels, *args, **kwargs),\n",
    "                        nn.BatchNorm2d(out_channels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetBasicBlock(ResNetResidualBlock):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_channels, out_channels, *args, **kwargs):\n",
    "        super().__init__(in_channels, out_channels, *args, **kwargs)\n",
    "        self.blocks = nn.Sequential(\n",
    "            conv_bn(self.in_channels, self.out_channels, conv=self.conv, bias=False, stride=self.downsampling),\n",
    "            activation_func(self.activation),\n",
    "            conv_bn(self.out_channels, self.expanded_channels, conv=self.conv, bias=False),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([1, 64, 10, 10])"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "dummy = torch.ones((1,32, 10, 10))\n",
    "\n",
    "block = ResNetBasicBlock(32, 64)\n",
    "block(dummy).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetBottleNeckBlock(ResNetResidualBlock):\n",
    "    expansion = 4\n",
    "    def __init__(self, in_channels, out_channels, *args, **kwargs):\n",
    "        super().__init__(in_channels, out_channels, expansion=4, *args, **kwargs)\n",
    "        self.blocks = nn.Sequential(\n",
    "            conv_bn(self.in_channels, self.out_channels, self.conv, kernel_size=1),\n",
    "            activation_func(self.activation),\n",
    "            conv_bn(self.out_channels, self.out_channels, self.conv, kernel_size=3, stride=self.downsampling),\n",
    "            activation_func(self.activation),\n",
    "            conv_bn(self.out_channels, self.expanded_channels, self.conv, kernel_size=1),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "ResNetBasicBlock(\n  (blocks): Sequential(\n    (0): Sequential(\n      (0): Conv2dAuto(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): ReLU(inplace=True)\n    (2): Sequential(\n      (0): Conv2dAuto(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (activate): ReLU(inplace=True)\n  (shortcut): Sequential(\n    (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n)\n"
    }
   ],
   "source": [
    "print(block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, block=ResNetBottleNeckBlock, n=1, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        downsampling = 2 if in_channels != out_channels else 1\n",
    "        self.blocks = nn.Sequential(\n",
    "            block(in_channels, out_channels, *args, **kwargs, downsampling=downsampling),\n",
    "            *[block(out_channels * block.expansion,\n",
    "                   out_channels, downsampling=1, *args, **kwargs) for _ in range(n-1)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.blocks(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([1, 512, 24, 24])"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "dummy = torch.ones((1,64, 48,48))\n",
    "\n",
    "layer = ResNetLayer(64, 128, block=ResNetBottleNeckBlock, n=1)\n",
    "layer(dummy).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetEncoder(nn.Module):\n",
    "    def __init__(self, in_channels=3, blocks_sizes=[64, 128, 256, 512], deepths=[1, 1, 1, 1], activation='relu',\n",
    "                 block=ResNetBottleNeckBlock, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.block_sizes = blocks_sizes\n",
    "\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, self.block_sizes[0], kernel_size=7, stride=2, padding=3, bias=False),\n",
    "            nn.BatchNorm2d(self.block_sizes[0]),\n",
    "            activation_func(activation),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "\n",
    "        self.in_out_block_sizes = list(zip(blocks_sizes, blocks_sizes[1:]))\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            ResNetLayer(blocks_sizes[0], blocks_sizes[0], n=deepths[0], activation=activation, block=block, *args,\n",
    "                        **kwargs),\n",
    "            *[ResNetLayer(in_channels * block.expansion, out_channels, n=n, activation=activation, block=block, *args,\n",
    "                          **kwargs)\n",
    "              for (in_channels, out_channels), n in zip(self.in_out_block_sizes, deepths[1:])\n",
    "              ]\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.gate(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetDecoder(nn.Module):\n",
    "    def __init__(self, in_features, n_classes):\n",
    "        super().__init__()\n",
    "        self.avg = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.decoder = nn.ModuleList([\n",
    "            nn.Linear(in_features, 500),\n",
    "            nn.Dropout2d(0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, 100),\n",
    "            nn.Dropout2d(0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 25),\n",
    "            nn.Dropout2d(0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(25,1),\n",
    "            nn.Sigmoid()\n",
    "        ])\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.avg(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        for block in self.decoder:\n",
    "            x = block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, in_channels, n_classes, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.encoder = ResNetEncoder(in_channels, *args, **kwargs)\n",
    "        self.decoder = ResNetDecoder(self.encoder.blocks[-1].blocks[-1].expanded_channels, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def res_conv1(in_channels, n_classes, block=ResNetBottleNeckBlock, *args, **kwargs):\n",
    "    return ResNet(in_channels, n_classes, block=block, deepths=[1,1,1,1], *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = res_conv1(1, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "ResNet(\n  (encoder): ResNetEncoder(\n    (gate): Sequential(\n      (0): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n      (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    )\n    (blocks): ModuleList(\n      (0): ResNetLayer(\n        (blocks): Sequential(\n          (0): ResNetBottleNeckBlock(\n            (blocks): Sequential(\n              (0): Sequential(\n                (0): Conv2dAuto(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              )\n              (1): ReLU(inplace=True)\n              (2): Sequential(\n                (0): Conv2dAuto(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              )\n              (3): ReLU(inplace=True)\n              (4): Sequential(\n                (0): Conv2dAuto(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              )\n            )\n            (activate): ReLU(inplace=True)\n            (shortcut): Sequential(\n              (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            )\n          )\n        )\n      )\n      (1): ResNetLayer(\n        (blocks): Sequential(\n          (0): ResNetBottleNeckBlock(\n            (blocks): Sequential(\n              (0): Sequential(\n                (0): Conv2dAuto(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              )\n              (1): ReLU(inplace=True)\n              (2): Sequential(\n                (0): Conv2dAuto(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              )\n              (3): ReLU(inplace=True)\n              (4): Sequential(\n                (0): Conv2dAuto(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              )\n            )\n            (activate): ReLU(inplace=True)\n            (shortcut): Sequential(\n              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            )\n          )\n        )\n      )\n      (2): ResNetLayer(\n        (blocks): Sequential(\n          (0): ResNetBottleNeckBlock(\n            (blocks): Sequential(\n              (0): Sequential(\n                (0): Conv2dAuto(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              )\n              (1): ReLU(inplace=True)\n              (2): Sequential(\n                (0): Conv2dAuto(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              )\n              (3): ReLU(inplace=True)\n              (4): Sequential(\n                (0): Conv2dAuto(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              )\n            )\n            (activate): ReLU(inplace=True)\n            (shortcut): Sequential(\n              (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n              (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            )\n          )\n        )\n      )\n      (3): ResNetLayer(\n        (blocks): Sequential(\n          (0): ResNetBottleNeckBlock(\n            (blocks): Sequential(\n              (0): Sequential(\n                (0): Conv2dAuto(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              )\n              (1): ReLU(inplace=True)\n              (2): Sequential(\n                (0): Conv2dAuto(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n                (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              )\n              (3): ReLU(inplace=True)\n              (4): Sequential(\n                (0): Conv2dAuto(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n              )\n            )\n            (activate): ReLU(inplace=True)\n            (shortcut): Sequential(\n              (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n              (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            )\n          )\n        )\n      )\n    )\n  )\n  (decoder): ResNetDecoder(\n    (avg): AdaptiveAvgPool2d(output_size=(1, 1))\n    (decoder): ModuleList(\n      (0): Linear(in_features=2048, out_features=500, bias=True)\n      (1): Dropout2d(p=0.5, inplace=False)\n      (2): ReLU()\n      (3): Linear(in_features=500, out_features=100, bias=True)\n      (4): Dropout2d(p=0.5, inplace=False)\n      (5): ReLU()\n      (6): Linear(in_features=100, out_features=25, bias=True)\n      (7): Dropout2d(p=0.5, inplace=False)\n      (8): ReLU()\n      (9): Linear(in_features=25, out_features=1, bias=True)\n      (10): Sigmoid()\n    )\n  )\n)\n"
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1           [-1, 64, 32, 32]           3,136\n       BatchNorm2d-2           [-1, 64, 32, 32]             128\n              ReLU-3           [-1, 64, 32, 32]               0\n         MaxPool2d-4           [-1, 64, 16, 16]               0\n            Conv2d-5          [-1, 256, 16, 16]          16,384\n       BatchNorm2d-6          [-1, 256, 16, 16]             512\n        Conv2dAuto-7           [-1, 64, 16, 16]           4,096\n       BatchNorm2d-8           [-1, 64, 16, 16]             128\n              ReLU-9           [-1, 64, 16, 16]               0\n       Conv2dAuto-10           [-1, 64, 16, 16]          36,864\n      BatchNorm2d-11           [-1, 64, 16, 16]             128\n             ReLU-12           [-1, 64, 16, 16]               0\n       Conv2dAuto-13          [-1, 256, 16, 16]          16,384\n      BatchNorm2d-14          [-1, 256, 16, 16]             512\n             ReLU-15          [-1, 256, 16, 16]               0\nResNetBottleNeckBlock-16          [-1, 256, 16, 16]               0\n      ResNetLayer-17          [-1, 256, 16, 16]               0\n           Conv2d-18            [-1, 512, 8, 8]         131,072\n      BatchNorm2d-19            [-1, 512, 8, 8]           1,024\n       Conv2dAuto-20          [-1, 128, 16, 16]          32,768\n      BatchNorm2d-21          [-1, 128, 16, 16]             256\n             ReLU-22          [-1, 128, 16, 16]               0\n       Conv2dAuto-23            [-1, 128, 8, 8]         147,456\n      BatchNorm2d-24            [-1, 128, 8, 8]             256\n             ReLU-25            [-1, 128, 8, 8]               0\n       Conv2dAuto-26            [-1, 512, 8, 8]          65,536\n      BatchNorm2d-27            [-1, 512, 8, 8]           1,024\n             ReLU-28            [-1, 512, 8, 8]               0\nResNetBottleNeckBlock-29            [-1, 512, 8, 8]               0\n      ResNetLayer-30            [-1, 512, 8, 8]               0\n           Conv2d-31           [-1, 1024, 4, 4]         524,288\n      BatchNorm2d-32           [-1, 1024, 4, 4]           2,048\n       Conv2dAuto-33            [-1, 256, 8, 8]         131,072\n      BatchNorm2d-34            [-1, 256, 8, 8]             512\n             ReLU-35            [-1, 256, 8, 8]               0\n       Conv2dAuto-36            [-1, 256, 4, 4]         589,824\n      BatchNorm2d-37            [-1, 256, 4, 4]             512\n             ReLU-38            [-1, 256, 4, 4]               0\n       Conv2dAuto-39           [-1, 1024, 4, 4]         262,144\n      BatchNorm2d-40           [-1, 1024, 4, 4]           2,048\n             ReLU-41           [-1, 1024, 4, 4]               0\nResNetBottleNeckBlock-42           [-1, 1024, 4, 4]               0\n      ResNetLayer-43           [-1, 1024, 4, 4]               0\n           Conv2d-44           [-1, 2048, 2, 2]       2,097,152\n      BatchNorm2d-45           [-1, 2048, 2, 2]           4,096\n       Conv2dAuto-46            [-1, 512, 4, 4]         524,288\n      BatchNorm2d-47            [-1, 512, 4, 4]           1,024\n             ReLU-48            [-1, 512, 4, 4]               0\n       Conv2dAuto-49            [-1, 512, 2, 2]       2,359,296\n      BatchNorm2d-50            [-1, 512, 2, 2]           1,024\n             ReLU-51            [-1, 512, 2, 2]               0\n       Conv2dAuto-52           [-1, 2048, 2, 2]       1,048,576\n      BatchNorm2d-53           [-1, 2048, 2, 2]           4,096\n             ReLU-54           [-1, 2048, 2, 2]               0\nResNetBottleNeckBlock-55           [-1, 2048, 2, 2]               0\n      ResNetLayer-56           [-1, 2048, 2, 2]               0\n    ResNetEncoder-57           [-1, 2048, 2, 2]               0\nAdaptiveAvgPool2d-58           [-1, 2048, 1, 1]               0\n           Linear-59                  [-1, 500]       1,024,500\n        Dropout2d-60                  [-1, 500]               0\n             ReLU-61                  [-1, 500]               0\n           Linear-62                  [-1, 100]          50,100\n        Dropout2d-63                  [-1, 100]               0\n             ReLU-64                  [-1, 100]               0\n           Linear-65                   [-1, 25]           2,525\n        Dropout2d-66                   [-1, 25]               0\n             ReLU-67                   [-1, 25]               0\n           Linear-68                    [-1, 1]              26\n          Sigmoid-69                    [-1, 1]               0\n    ResNetDecoder-70                    [-1, 1]               0\n================================================================\nTotal params: 9,086,815\nTrainable params: 9,086,815\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.02\nForward/backward pass size (MB): 10.67\nParams size (MB): 34.66\nEstimated Total Size (MB): 45.35\n----------------------------------------------------------------\n"
    }
   ],
   "source": [
    "summary(model.cuda(), (1,64,64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 700, 8568)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = transforms.Compose([transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from torch.optim import Adam, SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape((-1, 500, 700))\n",
    "Y = X.reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = X[:8000,:,:]\n",
    "train_Y = Y[:8000,:]\n",
    "test_X = X[8000:,:,:]\n",
    "test_Y = Y[8000:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, val_X, train_Y, val_Y = train_test_split(train_X, train_Y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_X.shape, train_Y.shape), (val_X.shape, val_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_X.reshape(-1, 1, 500, 700)\n",
    "train_X = torch.from_numpy(train_X)\n",
    "\n",
    "train_Y = train_Y.astype(int)\n",
    "train_Y = torch.from_numpy(train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_X =  val_X.reshape(-1, 1, 500, 700)\n",
    "val_X = torch.from_numpy(val_X)\n",
    "\n",
    "val_Y = val_Y.astype(int)\n",
    "val_Y = torch.from_numpy(val_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = res_conv1(1, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "criterion = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't run because CUDA is out of memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "    \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs):\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    # dataset\n",
    "    x_train, y_train = Variable(train_X), Variable(train_Y)\n",
    "    x_val, y_val = Variable(val_X), Variable(val_Y)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        x_train = x_train.cuda()\n",
    "        y_train = y_train.cuda()\n",
    "        x_val = x_val.cuda()\n",
    "        y_val = y_val.cuda()\n",
    "  \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    output_train = model(x_train)\n",
    "    output_val = model(x_val)\n",
    "    \n",
    "    loss_train = criterion(output_train, y_train)\n",
    "    loss_val = criterion(output_val, y_val)\n",
    "    \n",
    "    train_losses.append(loss_train)\n",
    "    val_losses.append(loss_val)\n",
    "    \n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    tr_loss = loss_train.item()\n",
    "    \n",
    "    if epochs % 2 == 0:\n",
    "        print('Epoch: ', epochs+1, '\\t', 'train loss: ', loss_train, '\\t', 'val loss: ', loss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epochs in range(n_epochs):\n",
    "    train(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('base': conda)",
   "language": "python",
   "name": "python361064bitbasecondac09563ef6808438fa176adb2f0efb54b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}