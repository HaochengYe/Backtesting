{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yfinance as yf\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import pytz\n",
    "import logging\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torch.optim import Adam, SGD\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dta_to_candlestick(data):\n",
    "    l = len(data)\n",
    "    # Make candlestick picture\n",
    "    layout = go.Layout(xaxis=dict(ticks='',\n",
    "                                  showgrid=False,\n",
    "                                  showticklabels=False,\n",
    "                                  rangeslider=dict(visible=False)),\n",
    "                       yaxis=dict(ticks='',\n",
    "                                  showgrid=False,\n",
    "                                  showticklabels=False),\n",
    "                       width=512,\n",
    "                       height=512,\n",
    "                       paper_bgcolor='rgba(0,0,0,0)',\n",
    "                       plot_bgcolor='rgba(0,0,0,0)')\n",
    "    fig = go.Figure(data=[go.Candlestick(x=np.linspace(1,l,l),\n",
    "                                         open=data.Open,\n",
    "                                         high=data.High,\n",
    "                                         low=data.Low,\n",
    "                                         close=data.Close)],\n",
    "                    layout=layout)\n",
    "    fig.write_image(\"images/fig-33.png\")\n",
    "\n",
    "    # Convert to numpy array\n",
    "    im = Image.open('images/fig-33.png')\n",
    "    #im = im.resize((300,300),Image.ANTIALIAS)\n",
    "    data = np.asarray(im)\n",
    "\n",
    "    # Return the first channel of the image\n",
    "    return data[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "START = datetime(1980, 1, 1)\n",
    "END = datetime(2020, 4, 23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = yf.Ticker('MSFT')\n",
    "hist = tic.history(start=START, end=END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dta_transformation(data, est_h):\n",
    "    # Make sure data has sufficient columns\n",
    "    assert 'Open' in data.columns\n",
    "    assert 'High' in data.columns\n",
    "    assert 'Low' in data.columns\n",
    "    assert 'Close' in data.columns\n",
    "\n",
    "    data['lag_close'] = data['Close'].shift(1)\n",
    "    data['Indicator'] = np.where(data['Close'] > data['lag_close'], 1, 0)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in range(est_h, data.shape[0]):\n",
    "        sub_dta = data.iloc[i - est_h:i]\n",
    "\n",
    "        y_i = data.iloc[i]['Indicator']\n",
    "        x_i = dta_to_candlestick(sub_dta)\n",
    "\n",
    "        y.append(y_i)\n",
    "        x.append(x_i)\n",
    "\n",
    "        print(\"{}/{}\".format(i - est_h, data.shape[0] - est_h))\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Dividends</th>\n",
       "      <th>Stock Splits</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2006-01-05</td>\n",
       "      <td>19.75</td>\n",
       "      <td>19.87</td>\n",
       "      <td>19.71</td>\n",
       "      <td>19.77</td>\n",
       "      <td>48245500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2006-01-06</td>\n",
       "      <td>19.69</td>\n",
       "      <td>19.77</td>\n",
       "      <td>19.40</td>\n",
       "      <td>19.71</td>\n",
       "      <td>100963000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2006-01-09</td>\n",
       "      <td>19.72</td>\n",
       "      <td>19.83</td>\n",
       "      <td>19.60</td>\n",
       "      <td>19.67</td>\n",
       "      <td>55625000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2006-01-10</td>\n",
       "      <td>19.52</td>\n",
       "      <td>19.79</td>\n",
       "      <td>19.47</td>\n",
       "      <td>19.77</td>\n",
       "      <td>64921900</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2006-01-11</td>\n",
       "      <td>19.78</td>\n",
       "      <td>20.06</td>\n",
       "      <td>19.70</td>\n",
       "      <td>19.99</td>\n",
       "      <td>70120700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2013-12-09</td>\n",
       "      <td>33.60</td>\n",
       "      <td>33.87</td>\n",
       "      <td>33.44</td>\n",
       "      <td>33.73</td>\n",
       "      <td>30286000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2013-12-10</td>\n",
       "      <td>33.65</td>\n",
       "      <td>33.90</td>\n",
       "      <td>33.13</td>\n",
       "      <td>33.21</td>\n",
       "      <td>37828600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2013-12-11</td>\n",
       "      <td>33.17</td>\n",
       "      <td>33.38</td>\n",
       "      <td>32.58</td>\n",
       "      <td>32.78</td>\n",
       "      <td>39853400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2013-12-12</td>\n",
       "      <td>32.80</td>\n",
       "      <td>32.80</td>\n",
       "      <td>32.40</td>\n",
       "      <td>32.44</td>\n",
       "      <td>36012800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2013-12-13</td>\n",
       "      <td>32.61</td>\n",
       "      <td>32.64</td>\n",
       "      <td>31.91</td>\n",
       "      <td>31.97</td>\n",
       "      <td>40066100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Open   High    Low  Close     Volume  Dividends  Stock Splits\n",
       "Date                                                                      \n",
       "2006-01-05  19.75  19.87  19.71  19.77   48245500        0.0           0.0\n",
       "2006-01-06  19.69  19.77  19.40  19.71  100963000        0.0           0.0\n",
       "2006-01-09  19.72  19.83  19.60  19.67   55625000        0.0           0.0\n",
       "2006-01-10  19.52  19.79  19.47  19.77   64921900        0.0           0.0\n",
       "2006-01-11  19.78  20.06  19.70  19.99   70120700        0.0           0.0\n",
       "...           ...    ...    ...    ...        ...        ...           ...\n",
       "2013-12-09  33.60  33.87  33.44  33.73   30286000        0.0           0.0\n",
       "2013-12-10  33.65  33.90  33.13  33.21   37828600        0.0           0.0\n",
       "2013-12-11  33.17  33.38  32.58  32.78   39853400        0.0           0.0\n",
       "2013-12-12  32.80  32.80  32.40  32.44   36012800        0.0           0.0\n",
       "2013-12-13  32.61  32.64  31.91  31.97   40066100        0.0           0.0\n",
       "\n",
       "[2000 rows x 7 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist.iloc[5000:7000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andyy\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\Andyy\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/990\n",
      "1/990\n",
      "2/990\n",
      "3/990\n",
      "4/990\n",
      "5/990\n",
      "6/990\n",
      "7/990\n",
      "8/990\n",
      "9/990\n",
      "10/990\n",
      "11/990\n",
      "12/990\n",
      "13/990\n",
      "14/990\n",
      "15/990\n",
      "16/990\n",
      "17/990\n",
      "18/990\n",
      "19/990\n",
      "20/990\n",
      "21/990\n",
      "22/990\n",
      "23/990\n",
      "24/990\n",
      "25/990\n",
      "26/990\n",
      "27/990\n",
      "28/990\n",
      "29/990\n",
      "30/990\n",
      "31/990\n",
      "32/990\n",
      "33/990\n",
      "34/990\n",
      "35/990\n",
      "36/990\n",
      "37/990\n",
      "38/990\n",
      "39/990\n",
      "40/990\n",
      "41/990\n",
      "42/990\n",
      "43/990\n",
      "44/990\n",
      "45/990\n",
      "46/990\n",
      "47/990\n",
      "48/990\n",
      "49/990\n",
      "50/990\n",
      "51/990\n",
      "52/990\n",
      "53/990\n",
      "54/990\n",
      "55/990\n",
      "56/990\n",
      "57/990\n",
      "58/990\n",
      "59/990\n",
      "60/990\n",
      "61/990\n",
      "62/990\n",
      "63/990\n",
      "64/990\n",
      "65/990\n",
      "66/990\n",
      "67/990\n",
      "68/990\n",
      "69/990\n",
      "70/990\n",
      "71/990\n",
      "72/990\n",
      "73/990\n",
      "74/990\n",
      "75/990\n",
      "76/990\n",
      "77/990\n",
      "78/990\n",
      "79/990\n",
      "80/990\n",
      "81/990\n",
      "82/990\n",
      "83/990\n",
      "84/990\n",
      "85/990\n",
      "86/990\n",
      "87/990\n",
      "88/990\n",
      "89/990\n",
      "90/990\n",
      "91/990\n",
      "92/990\n",
      "93/990\n",
      "94/990\n",
      "95/990\n",
      "96/990\n",
      "97/990\n",
      "98/990\n",
      "99/990\n",
      "100/990\n",
      "101/990\n",
      "102/990\n",
      "103/990\n",
      "104/990\n",
      "105/990\n",
      "106/990\n",
      "107/990\n",
      "108/990\n",
      "109/990\n",
      "110/990\n",
      "111/990\n",
      "112/990\n",
      "113/990\n",
      "114/990\n",
      "115/990\n",
      "116/990\n",
      "117/990\n",
      "118/990\n",
      "119/990\n",
      "120/990\n",
      "121/990\n",
      "122/990\n",
      "123/990\n",
      "124/990\n",
      "125/990\n",
      "126/990\n",
      "127/990\n",
      "128/990\n",
      "129/990\n",
      "130/990\n",
      "131/990\n",
      "132/990\n",
      "133/990\n",
      "134/990\n",
      "135/990\n",
      "136/990\n",
      "137/990\n",
      "138/990\n",
      "139/990\n",
      "140/990\n",
      "141/990\n",
      "142/990\n",
      "143/990\n",
      "144/990\n",
      "145/990\n",
      "146/990\n",
      "147/990\n",
      "148/990\n",
      "149/990\n",
      "150/990\n",
      "151/990\n",
      "152/990\n",
      "153/990\n",
      "154/990\n",
      "155/990\n",
      "156/990\n",
      "157/990\n",
      "158/990\n",
      "159/990\n",
      "160/990\n",
      "161/990\n",
      "162/990\n",
      "163/990\n",
      "164/990\n",
      "165/990\n",
      "166/990\n",
      "167/990\n",
      "168/990\n",
      "169/990\n",
      "170/990\n",
      "171/990\n",
      "172/990\n",
      "173/990\n",
      "174/990\n",
      "175/990\n",
      "176/990\n",
      "177/990\n",
      "178/990\n",
      "179/990\n",
      "180/990\n",
      "181/990\n",
      "182/990\n",
      "183/990\n",
      "184/990\n",
      "185/990\n",
      "186/990\n",
      "187/990\n",
      "188/990\n",
      "189/990\n",
      "190/990\n",
      "191/990\n",
      "192/990\n",
      "193/990\n",
      "194/990\n",
      "195/990\n",
      "196/990\n",
      "197/990\n",
      "198/990\n",
      "199/990\n",
      "200/990\n",
      "201/990\n",
      "202/990\n",
      "203/990\n",
      "204/990\n",
      "205/990\n",
      "206/990\n",
      "207/990\n",
      "208/990\n",
      "209/990\n",
      "210/990\n",
      "211/990\n",
      "212/990\n",
      "213/990\n",
      "214/990\n",
      "215/990\n",
      "216/990\n",
      "217/990\n",
      "218/990\n",
      "219/990\n",
      "220/990\n",
      "221/990\n",
      "222/990\n",
      "223/990\n",
      "224/990\n",
      "225/990\n",
      "226/990\n",
      "227/990\n",
      "228/990\n",
      "229/990\n",
      "230/990\n",
      "231/990\n",
      "232/990\n",
      "233/990\n",
      "234/990\n",
      "235/990\n",
      "236/990\n",
      "237/990\n",
      "238/990\n",
      "239/990\n",
      "240/990\n",
      "241/990\n",
      "242/990\n",
      "243/990\n",
      "244/990\n",
      "245/990\n",
      "246/990\n",
      "247/990\n",
      "248/990\n",
      "249/990\n",
      "250/990\n",
      "251/990\n",
      "252/990\n",
      "253/990\n",
      "254/990\n",
      "255/990\n",
      "256/990\n",
      "257/990\n",
      "258/990\n",
      "259/990\n",
      "260/990\n",
      "261/990\n",
      "262/990\n",
      "263/990\n",
      "264/990\n",
      "265/990\n",
      "266/990\n",
      "267/990\n",
      "268/990\n",
      "269/990\n",
      "270/990\n",
      "271/990\n",
      "272/990\n",
      "273/990\n",
      "274/990\n",
      "275/990\n",
      "276/990\n",
      "277/990\n",
      "278/990\n",
      "279/990\n",
      "280/990\n",
      "281/990\n",
      "282/990\n",
      "283/990\n",
      "284/990\n",
      "285/990\n",
      "286/990\n",
      "287/990\n",
      "288/990\n",
      "289/990\n",
      "290/990\n",
      "291/990\n",
      "292/990\n",
      "293/990\n",
      "294/990\n",
      "295/990\n",
      "296/990\n",
      "297/990\n",
      "298/990\n",
      "299/990\n",
      "300/990\n",
      "301/990\n",
      "302/990\n",
      "303/990\n",
      "304/990\n",
      "305/990\n",
      "306/990\n",
      "307/990\n",
      "308/990\n",
      "309/990\n",
      "310/990\n",
      "311/990\n",
      "312/990\n",
      "313/990\n",
      "314/990\n",
      "315/990\n",
      "316/990\n",
      "317/990\n",
      "318/990\n",
      "319/990\n",
      "320/990\n",
      "321/990\n",
      "322/990\n",
      "323/990\n",
      "324/990\n",
      "325/990\n",
      "326/990\n",
      "327/990\n",
      "328/990\n",
      "329/990\n",
      "330/990\n",
      "331/990\n",
      "332/990\n",
      "333/990\n",
      "334/990\n",
      "335/990\n",
      "336/990\n",
      "337/990\n",
      "338/990\n",
      "339/990\n",
      "340/990\n",
      "341/990\n",
      "342/990\n",
      "343/990\n",
      "344/990\n",
      "345/990\n",
      "346/990\n",
      "347/990\n",
      "348/990\n",
      "349/990\n",
      "350/990\n",
      "351/990\n",
      "352/990\n",
      "353/990\n",
      "354/990\n",
      "355/990\n",
      "356/990\n",
      "357/990\n",
      "358/990\n",
      "359/990\n",
      "360/990\n",
      "361/990\n",
      "362/990\n",
      "363/990\n",
      "364/990\n",
      "365/990\n",
      "366/990\n",
      "367/990\n",
      "368/990\n",
      "369/990\n",
      "370/990\n",
      "371/990\n",
      "372/990\n",
      "373/990\n",
      "374/990\n",
      "375/990\n",
      "376/990\n",
      "377/990\n",
      "378/990\n",
      "379/990\n",
      "380/990\n",
      "381/990\n",
      "382/990\n",
      "383/990\n",
      "384/990\n",
      "385/990\n",
      "386/990\n",
      "387/990\n",
      "388/990\n",
      "389/990\n",
      "390/990\n",
      "391/990\n",
      "392/990\n",
      "393/990\n",
      "394/990\n",
      "395/990\n",
      "396/990\n",
      "397/990\n",
      "398/990\n",
      "399/990\n",
      "400/990\n",
      "401/990\n",
      "402/990\n",
      "403/990\n",
      "404/990\n",
      "405/990\n",
      "406/990\n",
      "407/990\n",
      "408/990\n",
      "409/990\n",
      "410/990\n",
      "411/990\n",
      "412/990\n",
      "413/990\n",
      "414/990\n",
      "415/990\n",
      "416/990\n",
      "417/990\n",
      "418/990\n",
      "419/990\n",
      "420/990\n",
      "421/990\n",
      "422/990\n",
      "423/990\n",
      "424/990\n",
      "425/990\n",
      "426/990\n",
      "427/990\n",
      "428/990\n",
      "429/990\n",
      "430/990\n",
      "431/990\n",
      "432/990\n",
      "433/990\n",
      "434/990\n",
      "435/990\n",
      "436/990\n",
      "437/990\n",
      "438/990\n",
      "439/990\n",
      "440/990\n",
      "441/990\n",
      "442/990\n",
      "443/990\n",
      "444/990\n",
      "445/990\n",
      "446/990\n",
      "447/990\n",
      "448/990\n",
      "449/990\n",
      "450/990\n",
      "451/990\n",
      "452/990\n",
      "453/990\n",
      "454/990\n",
      "455/990\n",
      "456/990\n",
      "457/990\n",
      "458/990\n",
      "459/990\n",
      "460/990\n",
      "461/990\n",
      "462/990\n",
      "463/990\n",
      "464/990\n",
      "465/990\n",
      "466/990\n",
      "467/990\n",
      "468/990\n",
      "469/990\n",
      "470/990\n",
      "471/990\n",
      "472/990\n",
      "473/990\n",
      "474/990\n",
      "475/990\n",
      "476/990\n",
      "477/990\n",
      "478/990\n",
      "479/990\n",
      "480/990\n",
      "481/990\n",
      "482/990\n",
      "483/990\n",
      "484/990\n",
      "485/990\n",
      "486/990\n",
      "487/990\n",
      "488/990\n",
      "489/990\n",
      "490/990\n",
      "491/990\n",
      "492/990\n",
      "493/990\n",
      "494/990\n",
      "495/990\n",
      "496/990\n",
      "497/990\n",
      "498/990\n",
      "499/990\n",
      "500/990\n",
      "501/990\n",
      "502/990\n",
      "503/990\n",
      "504/990\n",
      "505/990\n",
      "506/990\n",
      "507/990\n",
      "508/990\n",
      "509/990\n",
      "510/990\n",
      "511/990\n",
      "512/990\n",
      "513/990\n",
      "514/990\n",
      "515/990\n",
      "516/990\n",
      "517/990\n",
      "518/990\n",
      "519/990\n",
      "520/990\n",
      "521/990\n",
      "522/990\n",
      "523/990\n",
      "524/990\n",
      "525/990\n",
      "526/990\n",
      "527/990\n",
      "528/990\n",
      "529/990\n",
      "530/990\n",
      "531/990\n",
      "532/990\n",
      "533/990\n",
      "534/990\n",
      "535/990\n",
      "536/990\n",
      "537/990\n",
      "538/990\n",
      "539/990\n",
      "540/990\n",
      "541/990\n",
      "542/990\n",
      "543/990\n",
      "544/990\n",
      "545/990\n",
      "546/990\n",
      "547/990\n",
      "548/990\n",
      "549/990\n",
      "550/990\n",
      "551/990\n",
      "552/990\n",
      "553/990\n",
      "554/990\n",
      "555/990\n",
      "556/990\n",
      "557/990\n",
      "558/990\n",
      "559/990\n",
      "560/990\n",
      "561/990\n",
      "562/990\n",
      "563/990\n",
      "564/990\n",
      "565/990\n",
      "566/990\n",
      "567/990\n",
      "568/990\n",
      "569/990\n",
      "570/990\n",
      "571/990\n",
      "572/990\n",
      "573/990\n",
      "574/990\n",
      "575/990\n",
      "576/990\n",
      "577/990\n",
      "578/990\n",
      "579/990\n",
      "580/990\n",
      "581/990\n",
      "582/990\n",
      "583/990\n",
      "584/990\n",
      "585/990\n",
      "586/990\n",
      "587/990\n",
      "588/990\n",
      "589/990\n",
      "590/990\n",
      "591/990\n",
      "592/990\n",
      "593/990\n",
      "594/990\n",
      "595/990\n",
      "596/990\n",
      "597/990\n",
      "598/990\n",
      "599/990\n",
      "600/990\n",
      "601/990\n",
      "602/990\n",
      "603/990\n",
      "604/990\n",
      "605/990\n",
      "606/990\n",
      "607/990\n",
      "608/990\n",
      "609/990\n",
      "610/990\n",
      "611/990\n",
      "612/990\n",
      "613/990\n",
      "614/990\n",
      "615/990\n",
      "616/990\n",
      "617/990\n",
      "618/990\n",
      "619/990\n",
      "620/990\n",
      "621/990\n",
      "622/990\n",
      "623/990\n",
      "624/990\n",
      "625/990\n",
      "626/990\n",
      "627/990\n",
      "628/990\n",
      "629/990\n",
      "630/990\n",
      "631/990\n",
      "632/990\n",
      "633/990\n",
      "634/990\n",
      "635/990\n",
      "636/990\n",
      "637/990\n",
      "638/990\n",
      "639/990\n",
      "640/990\n",
      "641/990\n",
      "642/990\n",
      "643/990\n",
      "644/990\n",
      "645/990\n",
      "646/990\n",
      "647/990\n",
      "648/990\n",
      "649/990\n",
      "650/990\n",
      "651/990\n",
      "652/990\n",
      "653/990\n",
      "654/990\n",
      "655/990\n",
      "656/990\n",
      "657/990\n",
      "658/990\n",
      "659/990\n",
      "660/990\n",
      "661/990\n",
      "662/990\n",
      "663/990\n",
      "664/990\n",
      "665/990\n",
      "666/990\n",
      "667/990\n",
      "668/990\n",
      "669/990\n",
      "670/990\n",
      "671/990\n",
      "672/990\n",
      "673/990\n",
      "674/990\n",
      "675/990\n",
      "676/990\n",
      "677/990\n",
      "678/990\n",
      "679/990\n",
      "680/990\n",
      "681/990\n",
      "682/990\n",
      "683/990\n",
      "684/990\n",
      "685/990\n",
      "686/990\n",
      "687/990\n",
      "688/990\n",
      "689/990\n",
      "690/990\n",
      "691/990\n",
      "692/990\n",
      "693/990\n",
      "694/990\n",
      "695/990\n",
      "696/990\n",
      "697/990\n",
      "698/990\n",
      "699/990\n",
      "700/990\n",
      "701/990\n",
      "702/990\n",
      "703/990\n",
      "704/990\n",
      "705/990\n",
      "706/990\n",
      "707/990\n",
      "708/990\n",
      "709/990\n",
      "710/990\n",
      "711/990\n",
      "712/990\n",
      "713/990\n",
      "714/990\n",
      "715/990\n",
      "716/990\n",
      "717/990\n",
      "718/990\n",
      "719/990\n",
      "720/990\n",
      "721/990\n",
      "722/990\n",
      "723/990\n",
      "724/990\n",
      "725/990\n",
      "726/990\n",
      "727/990\n",
      "728/990\n",
      "729/990\n",
      "730/990\n",
      "731/990\n",
      "732/990\n",
      "733/990\n",
      "734/990\n",
      "735/990\n",
      "736/990\n",
      "737/990\n",
      "738/990\n",
      "739/990\n",
      "740/990\n",
      "741/990\n",
      "742/990\n",
      "743/990\n",
      "744/990\n",
      "745/990\n",
      "746/990\n",
      "747/990\n",
      "748/990\n",
      "749/990\n",
      "750/990\n",
      "751/990\n",
      "752/990\n",
      "753/990\n",
      "754/990\n",
      "755/990\n",
      "756/990\n",
      "757/990\n",
      "758/990\n",
      "759/990\n",
      "760/990\n",
      "761/990\n",
      "762/990\n",
      "763/990\n",
      "764/990\n",
      "765/990\n",
      "766/990\n",
      "767/990\n",
      "768/990\n",
      "769/990\n",
      "770/990\n",
      "771/990\n",
      "772/990\n",
      "773/990\n",
      "774/990\n",
      "775/990\n",
      "776/990\n",
      "777/990\n",
      "778/990\n",
      "779/990\n",
      "780/990\n",
      "781/990\n",
      "782/990\n",
      "783/990\n",
      "784/990\n",
      "785/990\n",
      "786/990\n",
      "787/990\n",
      "788/990\n",
      "789/990\n",
      "790/990\n",
      "791/990\n",
      "792/990\n",
      "793/990\n",
      "794/990\n",
      "795/990\n",
      "796/990\n",
      "797/990\n",
      "798/990\n",
      "799/990\n",
      "800/990\n",
      "801/990\n",
      "802/990\n",
      "803/990\n",
      "804/990\n",
      "805/990\n",
      "806/990\n",
      "807/990\n",
      "808/990\n",
      "809/990\n",
      "810/990\n",
      "811/990\n",
      "812/990\n",
      "813/990\n",
      "814/990\n",
      "815/990\n",
      "816/990\n",
      "817/990\n",
      "818/990\n",
      "819/990\n",
      "820/990\n",
      "821/990\n",
      "822/990\n",
      "823/990\n",
      "824/990\n",
      "825/990\n",
      "826/990\n",
      "827/990\n",
      "828/990\n",
      "829/990\n",
      "830/990\n",
      "831/990\n",
      "832/990\n",
      "833/990\n",
      "834/990\n",
      "835/990\n",
      "836/990\n",
      "837/990\n",
      "838/990\n",
      "839/990\n",
      "840/990\n",
      "841/990\n",
      "842/990\n",
      "843/990\n",
      "844/990\n",
      "845/990\n",
      "846/990\n",
      "847/990\n",
      "848/990\n",
      "849/990\n",
      "850/990\n",
      "851/990\n",
      "852/990\n",
      "853/990\n",
      "854/990\n",
      "855/990\n",
      "856/990\n",
      "857/990\n",
      "858/990\n",
      "859/990\n",
      "860/990\n",
      "861/990\n",
      "862/990\n",
      "863/990\n",
      "864/990\n",
      "865/990\n",
      "866/990\n",
      "867/990\n",
      "868/990\n",
      "869/990\n",
      "870/990\n",
      "871/990\n",
      "872/990\n",
      "873/990\n",
      "874/990\n",
      "875/990\n",
      "876/990\n",
      "877/990\n",
      "878/990\n",
      "879/990\n",
      "880/990\n",
      "881/990\n",
      "882/990\n",
      "883/990\n",
      "884/990\n",
      "885/990\n",
      "886/990\n",
      "887/990\n",
      "888/990\n",
      "889/990\n",
      "890/990\n",
      "891/990\n",
      "892/990\n",
      "893/990\n",
      "894/990\n",
      "895/990\n",
      "896/990\n",
      "897/990\n",
      "898/990\n",
      "899/990\n",
      "900/990\n",
      "901/990\n",
      "902/990\n",
      "903/990\n",
      "904/990\n",
      "905/990\n",
      "906/990\n",
      "907/990\n",
      "908/990\n",
      "909/990\n",
      "910/990\n",
      "911/990\n",
      "912/990\n",
      "913/990\n",
      "914/990\n",
      "915/990\n",
      "916/990\n",
      "917/990\n",
      "918/990\n",
      "919/990\n",
      "920/990\n",
      "921/990\n",
      "922/990\n",
      "923/990\n",
      "924/990\n",
      "925/990\n",
      "926/990\n",
      "927/990\n",
      "928/990\n",
      "929/990\n",
      "930/990\n",
      "931/990\n",
      "932/990\n",
      "933/990\n",
      "934/990\n",
      "935/990\n",
      "936/990\n",
      "937/990\n",
      "938/990\n",
      "939/990\n",
      "940/990\n",
      "941/990\n",
      "942/990\n",
      "943/990\n",
      "944/990\n",
      "945/990\n",
      "946/990\n",
      "947/990\n",
      "948/990\n",
      "949/990\n",
      "950/990\n",
      "951/990\n",
      "952/990\n",
      "953/990\n",
      "954/990\n",
      "955/990\n",
      "956/990\n",
      "957/990\n",
      "958/990\n",
      "959/990\n",
      "960/990\n",
      "961/990\n",
      "962/990\n",
      "963/990\n",
      "964/990\n",
      "965/990\n",
      "966/990\n",
      "967/990\n",
      "968/990\n",
      "969/990\n",
      "970/990\n",
      "971/990\n",
      "972/990\n",
      "973/990\n",
      "974/990\n",
      "975/990\n",
      "976/990\n",
      "977/990\n",
      "978/990\n",
      "979/990\n",
      "980/990\n",
      "981/990\n",
      "982/990\n",
      "983/990\n",
      "984/990\n",
      "985/990\n",
      "986/990\n",
      "987/990\n",
      "988/990\n",
      "989/990\n"
     ]
    }
   ],
   "source": [
    "x, y = dta_transformation(hist.iloc[5000:6000], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.stack(x, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 512, 990)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = Image.fromarray(x[:,:,0])\n",
    "im.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_x = x[:,:,568:1568]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(990,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array(y)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_y = y[568:1568]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('test_1', x=sub_x, y=sub_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "568"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8568 % 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape[2] // 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    sub_x = x[:,:,(1000*i+568):(1000*(i+1)+568)]\n",
    "    sub_y = y[(1000*i+568):(1000*(i+1)+568)]\n",
    "    np.savez_compressed('test_{}'.format(i), x=sub_x, y=sub_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "333.7860107421875"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "350000000/1024/1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ResNet_CNN import *\n",
    "\n",
    "import math as m\n",
    "import torch\n",
    "from torch.nn import Linear, ReLU, Conv1d, Conv2d, Flatten, Sequential, CrossEntropyLoss, MSELoss, MaxPool1d, MaxPool2d, Dropout, BatchNorm1d, BatchNorm2d\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from functools import partial\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = res_conv1(1, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (encoder): ResNetEncoder(\n",
      "    (gate): Sequential(\n",
      "      (0): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (blocks): ModuleList(\n",
      "      (0): ResNetLayer(\n",
      "        (blocks): Sequential(\n",
      "          (0): ResNetBasicBlock(\n",
      "            (blocks): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Conv2dAuto(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "              (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "              (2): Sequential(\n",
      "                (0): Conv2dAuto(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (activate): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "            (shortcut): None\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): ResNetLayer(\n",
      "        (blocks): Sequential(\n",
      "          (0): ResNetBasicBlock(\n",
      "            (blocks): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Conv2dAuto(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "              (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "              (2): Sequential(\n",
      "                (0): Conv2dAuto(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (activate): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "            (shortcut): Sequential(\n",
      "              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): ResNetDecoder(\n",
      "    (avg): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (decoder): ModuleList(\n",
      "      (0): Linear(in_features=128, out_features=100, bias=True)\n",
      "      (1): Dropout2d(p=0.5, inplace=False)\n",
      "      (2): ReLU()\n",
      "      (3): Linear(in_features=100, out_features=25, bias=True)\n",
      "      (4): Dropout2d(p=0.5, inplace=False)\n",
      "      (5): ReLU()\n",
      "      (6): Linear(in_features=25, out_features=1, bias=True)\n",
      "      (7): Sigmoid()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─ResNetEncoder: 1-1                     [-1, 128, 64, 64]         --\n",
      "|    └─Sequential: 2-1                   [-1, 64, 128, 128]        --\n",
      "|    |    └─Conv2d: 3-1                  [-1, 64, 256, 256]        3,136\n",
      "|    |    └─BatchNorm2d: 3-2             [-1, 64, 256, 256]        128\n",
      "|    |    └─LeakyReLU: 3-3               [-1, 64, 256, 256]        --\n",
      "|    |    └─MaxPool2d: 3-4               [-1, 64, 128, 128]        --\n",
      "├─ResNetDecoder: 1-2                     [-1, 1]                   --\n",
      "|    └─AdaptiveAvgPool2d: 2-2            [-1, 128, 1, 1]           --\n",
      "==========================================================================================\n",
      "Total params: 322,843\n",
      "Trainable params: 322,843\n",
      "Non-trainable params: 0\n",
      "------------------------------------------------------------------------------------------\n",
      "Input size (MB): 1.00\n",
      "Forward/backward pass size (MB): 64.00\n",
      "Params size (MB): 1.23\n",
      "Estimated Total Size (MB): 66.23\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "------------------------------------------------------------------------------------------\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─ResNetEncoder: 1-1                     [-1, 128, 64, 64]         --\n",
      "|    └─Sequential: 2-1                   [-1, 64, 128, 128]        --\n",
      "|    |    └─Conv2d: 3-1                  [-1, 64, 256, 256]        3,136\n",
      "|    |    └─BatchNorm2d: 3-2             [-1, 64, 256, 256]        128\n",
      "|    |    └─LeakyReLU: 3-3               [-1, 64, 256, 256]        --\n",
      "|    |    └─MaxPool2d: 3-4               [-1, 64, 128, 128]        --\n",
      "├─ResNetDecoder: 1-2                     [-1, 1]                   --\n",
      "|    └─AdaptiveAvgPool2d: 2-2            [-1, 128, 1, 1]           --\n",
      "==========================================================================================\n",
      "Total params: 322,843\n",
      "Trainable params: 322,843\n",
      "Non-trainable params: 0\n",
      "------------------------------------------------------------------------------------------\n",
      "Input size (MB): 1.00\n",
      "Forward/backward pass size (MB): 64.00\n",
      "Params size (MB): 1.23\n",
      "Estimated Total Size (MB): 66.23\n",
      "------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(summary(model, (1,512,512)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_list = os.listdir('D:/GitHub/Backtesting/images_npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\GitHub\\\\Backtesting'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_dta = os.listdir('D:/GitHub/Backtesting/images_npy/{}'.format(ticker_list[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CI', 'DDS', 'IPG', 'LLY', 'MTG', 'NAVI', 'NKE', 'SEQ', 'WY']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticker_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CI_0.npz',\n",
       " 'CI_1.npz',\n",
       " 'CI_2.npz',\n",
       " 'CI_3.npz',\n",
       " 'CI_4.npz',\n",
       " 'CI_5.npz',\n",
       " 'CI_6.npz',\n",
       " 'CI_7.npz',\n",
       " 'CI_8.npz']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticker_dta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.load('C:/Users/Andyy/Documents/GitHub/Backtesting/images_npy/{}/{}'.format(ticker_list[0], ticker_dta[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dta_x = temp['x']\n",
    "dta_y = temp['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = res_conv1(1, 64)\n",
    "lr = 0.0001\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = np.load('test.npz')\n",
    "dta_x = loaded['x']\n",
    "dta_y = loaded['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((891, 512, 512), (891, 1)) ((99, 512, 512), (99, 1))\n"
     ]
    }
   ],
   "source": [
    "train_X, train_Y, val_X, val_Y = data_preprocessing(dta_x, dta_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(990,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dta_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('test', x=x, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataLoader(path):\n",
    "    loaded = np.load(path)\n",
    "    dta_x = loaded['x']\n",
    "    dta_y = loaded['y']\n",
    "    return dta_x, dta_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on CI_0.npz\n",
      "((900, 256, 256), (900, 1)) ((100, 256, 256), (100, 1))\n",
      "Epoch:  1 \t train loss:  tensor(0.7072, grad_fn=<BinaryCrossEntropyBackward>) \t val loss:  tensor(0.7103, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train on CI_1.npz\n",
      "((900, 256, 256), (900, 1)) ((100, 256, 256), (100, 1))\n",
      "Epoch:  1 \t train loss:  tensor(0.7052, grad_fn=<BinaryCrossEntropyBackward>) \t val loss:  tensor(0.7158, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train on CI_2.npz\n",
      "((900, 256, 256), (900, 1)) ((100, 256, 256), (100, 1))\n",
      "Epoch:  1 \t train loss:  tensor(0.7063, grad_fn=<BinaryCrossEntropyBackward>) \t val loss:  tensor(0.7024, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train on CI_3.npz\n",
      "((900, 256, 256), (900, 1)) ((100, 256, 256), (100, 1))\n",
      "Epoch:  1 \t train loss:  tensor(0.6973, grad_fn=<BinaryCrossEntropyBackward>) \t val loss:  tensor(0.7002, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train on CI_4.npz\n",
      "((900, 256, 256), (900, 1)) ((100, 256, 256), (100, 1))\n",
      "Epoch:  1 \t train loss:  tensor(0.6984, grad_fn=<BinaryCrossEntropyBackward>) \t val loss:  tensor(0.6930, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train on CI_5.npz\n",
      "((900, 256, 256), (900, 1)) ((100, 256, 256), (100, 1))\n",
      "Epoch:  1 \t train loss:  tensor(0.6921, grad_fn=<BinaryCrossEntropyBackward>) \t val loss:  tensor(0.6879, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train on CI_6.npz\n",
      "((900, 256, 256), (900, 1)) ((100, 256, 256), (100, 1))\n",
      "Epoch:  1 \t train loss:  tensor(0.6989, grad_fn=<BinaryCrossEntropyBackward>) \t val loss:  tensor(0.7002, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train on CI_7.npz\n",
      "((900, 256, 256), (900, 1)) ((100, 256, 256), (100, 1))\n",
      "Epoch:  1 \t train loss:  tensor(0.6953, grad_fn=<BinaryCrossEntropyBackward>) \t val loss:  tensor(0.6865, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train on CI_8.npz\n",
      "((900, 256, 256), (900, 1)) ((100, 256, 256), (100, 1))\n",
      "Epoch:  1 \t train loss:  tensor(0.6957, grad_fn=<BinaryCrossEntropyBackward>) \t val loss:  tensor(0.6966, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train on DDS_0.npz\n",
      "((900, 256, 256), (900, 1)) ((100, 256, 256), (100, 1))\n",
      "Epoch:  1 \t train loss:  tensor(0.7137, grad_fn=<BinaryCrossEntropyBackward>) \t val loss:  tensor(0.7180, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train on DDS_1.npz\n",
      "((900, 256, 256), (900, 1)) ((100, 256, 256), (100, 1))\n",
      "Epoch:  1 \t train loss:  tensor(0.7021, grad_fn=<BinaryCrossEntropyBackward>) \t val loss:  tensor(0.7007, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train on DDS_2.npz\n",
      "((900, 256, 256), (900, 1)) ((100, 256, 256), (100, 1))\n",
      "Epoch:  1 \t train loss:  tensor(0.6975, grad_fn=<BinaryCrossEntropyBackward>) \t val loss:  tensor(0.7088, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train on DDS_3.npz\n",
      "((900, 256, 256), (900, 1)) ((100, 256, 256), (100, 1))\n",
      "Epoch:  1 \t train loss:  tensor(0.7009, grad_fn=<BinaryCrossEntropyBackward>) \t val loss:  tensor(0.7019, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train on DDS_4.npz\n",
      "((900, 256, 256), (900, 1)) ((100, 256, 256), (100, 1))\n",
      "Epoch:  1 \t train loss:  tensor(0.6996, grad_fn=<BinaryCrossEntropyBackward>) \t val loss:  tensor(0.7060, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train on DDS_5.npz\n",
      "((900, 256, 256), (900, 1)) ((100, 256, 256), (100, 1))\n",
      "Epoch:  1 \t train loss:  tensor(0.6959, grad_fn=<BinaryCrossEntropyBackward>) \t val loss:  tensor(0.7065, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train on DDS_6.npz\n",
      "((900, 256, 256), (900, 1)) ((100, 256, 256), (100, 1))\n",
      "Epoch:  1 \t train loss:  tensor(0.6959, grad_fn=<BinaryCrossEntropyBackward>) \t val loss:  tensor(0.6940, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train on DDS_7.npz\n",
      "((900, 256, 256), (900, 1)) ((100, 256, 256), (100, 1))\n",
      "Epoch:  1 \t train loss:  tensor(0.6905, grad_fn=<BinaryCrossEntropyBackward>) \t val loss:  tensor(0.6866, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train on DDS_8.npz\n",
      "((900, 256, 256), (900, 1)) ((100, 256, 256), (100, 1))\n",
      "Epoch:  1 \t train loss:  tensor(0.6950, grad_fn=<BinaryCrossEntropyBackward>) \t val loss:  tensor(0.6877, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train on DDS_9.npz\n",
      "((900, 256, 256), (900, 1)) ((100, 256, 256), (100, 1))\n",
      "Epoch:  1 \t train loss:  tensor(0.6940, grad_fn=<BinaryCrossEntropyBackward>) \t val loss:  tensor(0.6959, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train on IPG_0.npz\n",
      "((900, 256, 256), (900, 1)) ((100, 256, 256), (100, 1))\n",
      "Epoch:  1 \t train loss:  tensor(0.7279, grad_fn=<BinaryCrossEntropyBackward>) \t val loss:  tensor(0.7291, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train on IPG_1.npz\n",
      "((900, 256, 256), (900, 1)) ((100, 256, 256), (100, 1))\n",
      "Epoch:  1 \t train loss:  tensor(0.7107, grad_fn=<BinaryCrossEntropyBackward>) \t val loss:  tensor(0.7014, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train on IPG_2.npz\n",
      "((900, 256, 256), (900, 1)) ((100, 256, 256), (100, 1))\n",
      "Epoch:  1 \t train loss:  tensor(0.7010, grad_fn=<BinaryCrossEntropyBackward>) \t val loss:  tensor(0.6975, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train on IPG_3.npz\n",
      "((900, 256, 256), (900, 1)) ((100, 256, 256), (100, 1))\n",
      "Epoch:  1 \t train loss:  tensor(0.6998, grad_fn=<BinaryCrossEntropyBackward>) \t val loss:  tensor(0.6918, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train on IPG_4.npz\n",
      "((900, 256, 256), (900, 1)) ((100, 256, 256), (100, 1))\n",
      "Epoch:  1 \t train loss:  tensor(0.6949, grad_fn=<BinaryCrossEntropyBackward>) \t val loss:  tensor(0.6933, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train on IPG_5.npz\n",
      "((900, 256, 256), (900, 1)) ((100, 256, 256), (100, 1))\n",
      "Epoch:  1 \t train loss:  tensor(0.6945, grad_fn=<BinaryCrossEntropyBackward>) \t val loss:  tensor(0.6873, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train on IPG_6.npz\n",
      "((900, 256, 256), (900, 1)) ((100, 256, 256), (100, 1))\n",
      "Epoch:  1 \t train loss:  tensor(0.6952, grad_fn=<BinaryCrossEntropyBackward>) \t val loss:  tensor(0.6954, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train on IPG_7.npz\n",
      "((900, 256, 256), (900, 1)) ((100, 256, 256), (100, 1))\n",
      "Epoch:  1 \t train loss:  tensor(0.6965, grad_fn=<BinaryCrossEntropyBackward>) \t val loss:  tensor(0.6922, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train on IPG_8.npz\n",
      "((900, 256, 256), (900, 1)) ((100, 256, 256), (100, 1))\n",
      "Epoch:  1 \t train loss:  tensor(0.6944, grad_fn=<BinaryCrossEntropyBackward>) \t val loss:  tensor(0.6971, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train on IPG_9.npz\n",
      "((900, 256, 256), (900, 1)) ((100, 256, 256), (100, 1))\n",
      "Epoch:  1 \t train loss:  tensor(0.6929, grad_fn=<BinaryCrossEntropyBackward>) \t val loss:  tensor(0.6889, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train on LLY_0.npz\n",
      "((900, 256, 256), (900, 1)) ((100, 256, 256), (100, 1))\n",
      "Epoch:  1 \t train loss:  tensor(0.7062, grad_fn=<BinaryCrossEntropyBackward>) \t val loss:  tensor(0.7019, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Train on LLY_1.npz\n",
      "((900, 256, 256), (900, 1)) ((100, 256, 256), (100, 1))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at ..\\c10\\core\\CPUAllocator.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 471859200 bytes. Buy new RAM!\n(no backtrace available)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-f482e1fd9ff4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-f1e3f7e9326a>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(epochs)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mval_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mloss_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m         \"\"\"\n\u001b[1;32m--> 198\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at ..\\c10\\core\\CPUAllocator.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 471859200 bytes. Buy new RAM!\n(no backtrace available)"
     ]
    }
   ],
   "source": [
    "for comp in ticker_list:\n",
    "    ticker_dta = os.listdir('D:/GitHub/Backtesting/images_npy/{}'.format(comp))\n",
    "    for dta in ticker_dta:\n",
    "        try:\n",
    "            path = 'D:/GitHub/Backtesting/images_npy/{}/{}'.format(comp, dta)\n",
    "            dta_x, dta_y = dataLoader(path)\n",
    "            print(\"Train on {}\".format(dta))\n",
    "            # Begin training\n",
    "            \n",
    "            train_X, train_Y, val_X, val_Y = data_preprocessing(dta_x, dta_y)\n",
    "            gc.collect()\n",
    "            \n",
    "            train(0)\n",
    "        except RuntimeError:\n",
    "            model_path = './cnn_res.pth'\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            print(\"{} breaks the computer!!!\".format(dta))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './cnn_res.pth'\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = res_conv1(1,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(X, Y):\n",
    "    X = X.reshape((-1, 512, 512)).astype(np.float32)\n",
    "    Y = Y.reshape((-1, 1)).astype(np.float32)\n",
    "    train_X, val_X, train_Y, val_Y = train_test_split(X, Y, test_size=0.1, random_state=42)\n",
    "    print((train_X.shape, train_Y.shape), (val_X.shape, val_Y.shape))\n",
    "\n",
    "    train_X = train_X.reshape(-1, 1, 512, 512)\n",
    "    train_X = torch.from_numpy(train_X)\n",
    "    train_Y = torch.from_numpy(train_Y)\n",
    "\n",
    "    val_X =  val_X.reshape(-1, 1, 512, 512)\n",
    "    val_X = torch.from_numpy(val_X)\n",
    "    val_Y = torch.from_numpy(val_Y)\n",
    "    \n",
    "    return train_X, train_Y, val_X, val_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = transforms.Compose([transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((900, 256, 256), (900, 1))((100, 256, 256), (100, 1))\n"
     ]
    }
   ],
   "source": [
    "train_X, train_Y, val_X, val_Y = data_preprocessing(dta_x, dta_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = res_conv1(1, 64)\n",
    "lr = 0.0001\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs):\n",
    "    # dataset\n",
    "    x_train, y_train = Variable(train_X), Variable(train_Y)\n",
    "    x_val, y_val = Variable(val_X), Variable(val_Y)\n",
    " \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    output_train = model(x_train)\n",
    "    output_val = model(x_val)\n",
    "    \n",
    "    loss_train = criterion(output_train, y_train.type(torch.float))\n",
    "    loss_val = criterion(output_val, y_val.type(torch.float))\n",
    "    \n",
    "    train_losses.append(loss_train)\n",
    "    val_losses.append(loss_val)\n",
    "    \n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print('Epoch: ', epochs+1, '\\t', 'train loss: ', loss_train, '\\t', 'val loss: ', loss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breaks the computer!!!\n"
     ]
    }
   ],
   "source": [
    "model_path = './cnn_res-1.pth'\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(\"Breaks the computer!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breaks the computer!!!\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 2\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "tr_loss = 0\n",
    "try:\n",
    "    for epochs in range(n_epochs):\n",
    "        train(epochs)\n",
    "except RuntimeError:\n",
    "    model_path = './cnn_res-1.pth'\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(\"Breaks the computer!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
